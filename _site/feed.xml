<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-12-16T08:28:52+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Solar Grammars</title><subtitle></subtitle><author><name></name></author><entry><title type="html">Grounding Color Comparatives</title><link href="http://localhost:4000/blog/grounding-color-comparatives.html" rel="alternate" type="text/html" title="Grounding Color Comparatives" /><published>2019-08-01T00:00:00+09:00</published><updated>2019-08-01T00:00:00+09:00</updated><id>http://localhost:4000/blog/grounding-color-comparatives</id><content type="html" xml:base="http://localhost:4000/blog/grounding-color-comparatives.html">&lt;p&gt;The methods  we have covered so far allowed  us to learn  a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when
we say a given color is &lt;code class=&quot;highlighter-rouge&quot;&gt;mustard yellow&lt;/code&gt;, we are basically relating our experienced notion of &lt;code class=&quot;highlighter-rouge&quot;&gt;mustard&lt;/code&gt;  and a reference &lt;code class=&quot;highlighter-rouge&quot;&gt;yellow&lt;/code&gt; and associating them
to express our perception through language. Another example can be seen when we say &lt;code class=&quot;highlighter-rouge&quot;&gt;dark green&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;deep blue&lt;/code&gt;. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of  &lt;code class=&quot;highlighter-rouge&quot;&gt;dark green&lt;/code&gt;, we use our preconceived reference of what we understand as &lt;code class=&quot;highlighter-rouge&quot;&gt;green&lt;/code&gt; and its &lt;code class=&quot;highlighter-rouge&quot;&gt;dark&lt;/code&gt; variant. In this process, we are implicitly internalizing that the color we are perceiving is &lt;code class=&quot;highlighter-rouge&quot;&gt;darker&lt;/code&gt; than 
the reference we have.&lt;/p&gt;

&lt;p&gt;It could be interesting to model or quantify the distance between a predefined reference and a newly perceived color, and how such distance condition the language we use to  describe it.&lt;/p&gt;

&lt;p&gt;In an &lt;a href=&quot;https://www.aclweb.org/anthology/P18-2125&quot;&gt;ACL’18 paper&lt;/a&gt;, Winn et al. proposed a way to ground color comparatives as vectors in a color space. Let’s assume we have a reference color &lt;code class=&quot;highlighter-rouge&quot;&gt;yellow&lt;/code&gt; and a given target color &lt;code class=&quot;highlighter-rouge&quot;&gt;dark yellow&lt;/code&gt;, both expressed as RGB points. The associated comparative in this case is &lt;code class=&quot;highlighter-rouge&quot;&gt;darker&lt;/code&gt;, as naturally, &lt;code class=&quot;highlighter-rouge&quot;&gt;dark yellow&lt;/code&gt;
 is &lt;em&gt;darker&lt;/em&gt; than &lt;code class=&quot;highlighter-rouge&quot;&gt;yellow&lt;/code&gt; :). The key idea is to represent a comparative between both colors as a vector rooted at the reference  and that points in the direction of the the target.&lt;/p&gt;

&lt;p&gt;The first interesting thing about that work is the construction of an ad-hoc dataset. The authors take as starting point  the dataset from &lt;a href=&quot;https://www.aclweb.org/anthology/Q15-1008&quot;&gt;McMahan et al.&lt;/a&gt; (which in turn is a processed version of the original &lt;a href=&quot;https://blog.xkcd.com/2010/05/03/color-survey-results/&quot;&gt;XKCD color survey&lt;/a&gt;). This dataset associates a color description with a RGB point.&lt;/p&gt;

&lt;p&gt;The authors of this paper noticed that a considerable amount of descriptions contain
adjectives, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;light green&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;dark purple&lt;/code&gt;. From those adjectives, they obtained the associated comparatives (  &lt;code class=&quot;highlighter-rouge&quot;&gt;light&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;lighter&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;dark&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;darker&lt;/code&gt;) so they can
conform tuples like  &lt;code class=&quot;highlighter-rouge&quot;&gt;(green, darker, dark green)&lt;/code&gt;, where 
the first element represent the reference color as RGB tuple, the second is the associated comparative, and third element is the target color, also expressed as a  RGB tuple.&lt;/p&gt;

&lt;p&gt;Let’s take a look a some tuples and their associated points, by plotting a two dimensional reduction via T-SNE. For example, in the following figure we can see how the RGB points for &lt;code class=&quot;highlighter-rouge&quot;&gt;lavender&lt;/code&gt; are positioned  in relation to its available comparatives.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_lavender.png&quot; alt=&quot;lavender&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing we can immediately notice is that … some does not quite resemble the common notion of &lt;code class=&quot;highlighter-rouge&quot;&gt;lavender&lt;/code&gt;. But that is an interesting aspect of the dataset, as such categorization comes from a real survey, i.e., it compresses the diversity of perception from a huge number of people.  I added a marker associated to the mean value of comparative (&lt;code class=&quot;highlighter-rouge&quot;&gt;average&lt;/code&gt; in this case means just plain &lt;code class=&quot;highlighter-rouge&quot;&gt;lavender&lt;/code&gt;) and we can easy distinguish the defined color zones. Interestingly, the &lt;code class=&quot;highlighter-rouge&quot;&gt;light lavender&lt;/code&gt; seems to be quite different from the rest, but between elements in this group, we still can see distances quite considerable. If we take a look at a couple more of examples,  we see a consistent distinction:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rose&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_rose.png&quot; alt=&quot;mint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;turquoise&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_turquoise.png&quot; alt=&quot;mint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;violet&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_violet.png&quot; alt=&quot;peach&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Something worth noticing there could be several ways to structure  a training set. In the case of the paper from Winn et al.,  instead of forming all possible reference-target pairs between RGB points, they assume that the references converge, therefore only the average values of those sets are used.&lt;/p&gt;

&lt;p&gt;The model consists of a linear layer that receives the concatenation of the  reference color &lt;script type=&quot;math/tex&quot;&gt;r_c&lt;/script&gt; (as RGB three-dimensional vector) and an embedding associated to the comparative &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; (in the paper the authors used the  Google’s 300-dimensional word2vec pretrained vectors). The resulting representation is fed to an additional layer along with the reference color and the output consists of a 3 dimensional vector &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; that represents the direction in the color space of the vector rooted at &lt;script type=&quot;math/tex&quot;&gt;r_c&lt;/script&gt; and the &lt;em&gt;follows&lt;/em&gt; the direction towards the target &lt;script type=&quot;math/tex&quot;&gt;t_c&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The loss function has two components, the first one computes the cosine distance between the learned vector &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; and the vector that connects the reference and the target colors ( &lt;script type=&quot;math/tex&quot;&gt;\vec{tr} =  t_c - r_c&lt;/script&gt;).
The second component compares the size of the learned vector against the size of the vector associated to the target color. With this, it is expected to modulate  the distance to the target color so that the colors in the line of the vector resemble the comparative.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some examples of learned comparative vectors.  In the case of the tuple &lt;code class=&quot;highlighter-rouge&quot;&gt;(lavender, darker, dark lavender)&lt;/code&gt; the first figure shows the differences between the &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\vec{tr}&lt;/script&gt; as we graph them in a 
three dimensional space. As RGB is not a well defined metric space, the colors were transformed into the LAB
space. The second figure shows the the color gradient associated to the &lt;script type=&quot;math/tex&quot;&gt;\vec{tr}&lt;/script&gt; vector and the one associated to &lt;script type=&quot;math/tex&quot;&gt;\vec{v_g}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/vector_visualization_darklavender.png&quot; alt=&quot;vvis&quot; /&gt;  &lt;img src=&quot;/assets/img/blog/color-comparatives/color_gradients_darklavender.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the figures above, we can see that the model actually does a good job, as &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; follows 
quite close the direction of &lt;script type=&quot;math/tex&quot;&gt;\vec{tr}&lt;/script&gt;. In terms of magnitude, &lt;script type=&quot;math/tex&quot;&gt;\vec{wg}&lt;/script&gt; is longer, which may say something
about the need to adjust the second term of the loss function. But in practical terms, if we look at the color
gradients, we can see that &lt;script type=&quot;math/tex&quot;&gt;\vec{wg}&lt;/script&gt; also takes us to a state that we could describe as  &lt;code class=&quot;highlighter-rouge&quot;&gt;dark lavender&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If we inspect other pairs, we can see the a similar behavior.&lt;/p&gt;

&lt;p&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;(yellow, more orange, orange yellow)&lt;/code&gt;, we see a quite close fit in terms of the direction, while at the same time a bigger difference 
in term of the magnitudes. This disagreement seems to not make much difference regarding the colors we can obtain. 
&lt;img src=&quot;/assets/img/blog/color-comparatives/vector_visualization_orangeyellow.png&quot; alt=&quot;vvis&quot; /&gt;  &lt;img src=&quot;/assets/img/blog/color-comparatives/color_gradients_orangeyellow.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;(mint, darker, dark mint)&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/vector_visualization_darkmint.png&quot; alt=&quot;vvis&quot; /&gt;  &lt;img src=&quot;/assets/img/blog/color-comparatives/color_gradients_darkmint.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In general,  the method works quite well for tuples on the test set whose elements have appeared during training, which means
RGB  tuples that are in the proximity of colors already seen. Naturally, things get more interesting when we pass a tuple in which  one or more components are totally out of distribution.&lt;/p&gt;

&lt;p&gt;For example, lets take a look at the tuple &lt;code class=&quot;highlighter-rouge&quot;&gt;(pale blue, more , very pale blue)&lt;/code&gt;. In this case &lt;code class=&quot;highlighter-rouge&quot;&gt;pale blue&lt;/code&gt; was not present 
on the training data. If we compute the color gradients using 
the grounded comparative vector, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/unseen_ref_color_gradients_verypaleblue.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see a clear difference on the ending color both gradients
reach, with the one associated to &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; getting closer to what we could consider &lt;em&gt;muddier&lt;/em&gt; rather than &lt;em&gt;very pale&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Another setting we can explore is when the comparative was not seen during training. For example,  for the tuple &lt;code class=&quot;highlighter-rouge&quot;&gt;(pink, purplish, purplish pink)&lt;/code&gt;, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/unseen_comp_color_gradients_purplishpink.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we can see how the model is having a really hard time
trying to align we the ground truth. In fact,  we can see the 
ending colors do not resemble at all, and  in the case of
the color gradient associated to &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt;, there is no sign 
of  &lt;em&gt;purplishness&lt;/em&gt; :)&lt;/p&gt;

&lt;p&gt;This tells us a lot about how important is for the model
to handle the comparative representations. As we reviewed above, in the original paper the vectors associated to the comparative
tokens are pretrained vectors from Google Word2vec. Of course replacing those vectors with other, more refined, pretrainied sources could allow more generalization, but I wonder if there is any way to consider out of vocabulary comparatives in a more comprehensive way.&lt;/p&gt;

&lt;p&gt;One way to approach such problem could be to estimate the direction of an unseen comparative based on the semantic relationship it may  have with existing comparatives. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;brighter&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;lighter&lt;/code&gt; , while not pointing in the exact same direction, probably are close enough. Therefore, if one of them  not appear, probably their RGB points could 
make the model be aware of their relationship, guiding it towards a feasible region in the  color space. Unfortunately, such relationship is in directly encoded in the pretrained vectors so it might be necessary to induce such bias. Besides that, I think the most interesting extension could be 
to incorporate structure  (hierarchies ? ) to the comparative relationships, maybe based on an external source. But lets keep
that as a future work for the reader.&lt;/p&gt;

&lt;p&gt;All the code, data generation and  figures can be found here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://github.com/solargrammars/comparative_color_descriptions&quot;&gt;http://github.com/solargrammars/comparative_color_descriptions&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Solar Grammars</name></author><category term="blog" /><summary type="html">The methods we have covered so far allowed us to learn a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when we say a given color is mustard yellow, we are basically relating our experienced notion of mustard and a reference yellow and associating them to express our perception through language. Another example can be seen when we say dark green or deep blue. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of dark green, we use our preconceived reference of what we understand as green and its dark variant. In this process, we are implicitly internalizing that the color we are perceiving is darker than the reference we have.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/arctic-1.jpg" /></entry><entry><title type="html">Learning Color-Language Representations</title><link href="http://localhost:4000/blog/learning-color-language-representations.html" rel="alternate" type="text/html" title="Learning Color-Language Representations" /><published>2019-06-22T00:00:00+09:00</published><updated>2019-06-22T00:00:00+09:00</updated><id>http://localhost:4000/blog/learning-color-language-representations</id><content type="html" xml:base="http://localhost:4000/blog/learning-color-language-representations.html">&lt;p&gt;Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based 
on that, contribute to understanding the principles of language 
acquisition. In that sense, color description, or more generally,  the generation of color language, is a relevant aspect for  understanding the emergence of human expression.&lt;/p&gt;

&lt;p&gt;The use of colors as constructs to describe situations can be seen 
in the literature. Some uses are intriguing, for example&lt;br /&gt;
William Gibson’s Neuromancer begins with “&lt;em&gt;The sky above the 
port was the color of television, turned to a dead channel&lt;/em&gt;”. What 
exactly did the author want to express here? Of course, there is a 
common agreement on the  purpose of such a sentence as a way of 
setting the mood of a cyberpunk novel, but the perception that each 
reader has certainly varies given the situatedness of language.&lt;/p&gt;

&lt;p&gt;Another example is H.P. Lovecraft, who constantly uses the concept of color (or the lack of it) to describe the universes where his solitary characters are transported to:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A rather large congeries of iridescent, prolately spheroidal bubbles and a very much smaller polyhedron of &lt;strong&gt;unknown colours&lt;/strong&gt; and rapidly shifting surface angles (The Dreams of the House of the Witch)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The vast tomb, or temple, was an &lt;strong&gt;anomalous color&lt;/strong&gt; — a nameless blue-violet shade (The Tree on the Hill)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Light filtered down from asky of &lt;strong&gt;no assignable colour&lt;/strong&gt; in baffling, contradictory directions, and played almost sentiently over what seemed to be a curved line of gigantic hieroglyphed pedestals more hexagonal than otherwise and surmounted by cloaked, ill-defined Shapes (Through the Gates of the Silver Key).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For Lovecraft,  the inability to identify or generate a meaningful description of color seems to be the unequivocal sign that something very bad, as usual, is going to happen. Like if being able to define a color meant for an individual to have control over the understanding of the reality.&lt;/p&gt;

&lt;p&gt;Besides the literary connotations, from a machine learning point of view, there is a whole line of research dedicated to this problem, which combines natural language processing, psychology and, even art theory.&lt;/p&gt;

&lt;p&gt;In general, the main idea is to use a set of (description, color) pairs to train a model able to map between these modalities. The used models differ in their nature but the task remains almost the same.  For example, we can mention the work of &lt;a href=&quot;https://aclweb.org/anthology/D16-1202&quot;&gt;Kawakami et al.&lt;/a&gt; which uses a character-level recurrent architecture to map a color description to an associated color. This tackles the particular problem that word-level models usually face, which is that color descriptions tend to be short, limiting the expressiveness of the models. &lt;a href=&quot;https://web.stanford.edu/class/cs224n/reports/2762012.pdf&quot;&gt;Bhargava et al.&lt;/a&gt; propose an autoencoder where the latent feature representation is aimed to align with a three-dimensional color vector. &lt;a href=&quot;https://aclweb.org/anthology/D16-1243&quot;&gt;Moore et al.&lt;/a&gt; focuses on the compositionality of the color descriptions by combining a neural encoder with a Fourier-based color transformer. The &lt;a href=&quot;https://transacl.org/ojs/index.php/tacl/article/view/1142&quot;&gt;same author&lt;/a&gt; incorporates a context perspective by modeling the description generation in a speaker-listener setting and also has recently &lt;a href=&quot;https://arxiv.org/pdf/1803.03917.pdf&quot;&gt;published&lt;/a&gt; an approach that incorporates bilingual data.&lt;/p&gt;

&lt;h3 id=&quot;from-language-to-colors&quot;&gt;From language to colors&lt;/h3&gt;

&lt;p&gt;Let’s start analyzing  Kawakami et al. paper, replicating the results and discussing possible directions. 
The first element to consider is the data. In this case, the authors propose to extract color-descriptions pairs from Colourlovers. Fortunately, there is an &lt;a href=&quot;http://www.colourlovers.com/api&quot;&gt;API&lt;/a&gt; and even a &lt;a href=&quot;https://github.com/elbaschid/python-colourlovers&quot;&gt;Python client&lt;/a&gt; and the extraction can be done with something like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;colourlovers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColourLovers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColourLovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cont&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# take it easy&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;cont&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;green&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After some hours, I was able to get around 800.000 pairs. I applied certain filters to remove duplicates and  characters that were very infrequent, leading to a set with the following characteristics:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/1.png&quot; alt=&quot;char&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/2.png&quot; alt=&quot;len&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The associated colors are transformed from RGB to Lab format, which allows us to arrange  in a 3-dimensional space. The spatial disposition of a sample is presented in the following animated graph:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;animate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# for a sample of 5000 colors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# initial angle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;facecolors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ani&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;animation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FuncAnimation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;animate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frames&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ani&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lab_space_90_30fps.gif'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'imagemagick'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/lab_space_90_30fps.gif&quot; alt=&quot;3dlab&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regarding the model, it consists of two main blocks. The first one, a color description encoder, takes a description of a color in natural language and pass it through a character-level LSTM, from which the last hidden state, &lt;script type=&quot;math/tex&quot;&gt;h \in \mathbb{R}^{300}&lt;/script&gt;  is used as a description learned feature vector. The second one, a feed-forward layer takes &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; and outputs the associated color in Lab format as &lt;script type=&quot;math/tex&quot;&gt;\hat{y} = \sigma(Wh+ b)&lt;/script&gt;, with &lt;script type=&quot;math/tex&quot;&gt;W \in \mathbb{R}^{3 \times 300}&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;b \in \mathbb{R}^3&lt;/script&gt;.
The model tries to minimize the mean squared error between the output color and the reference using backpropagation. While I tried several optimizers, Adam was the one with a more consistent performance on  the validation set, as seen in the following comparison:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/curves.png&quot; alt=&quot;curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial results show coherence between the generated colors and their references. If we analyze some of the color transitions as the network passes through the descriptions, we can see how intermediate colors are also aligned with the incomplete descriptions. This shows the expressive power of the character level approach, as the generation is a sequential process at a more fine grained granularity.&lt;/p&gt;

&lt;p&gt;Now, let’s take a look at  how  the hidden vectors  &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; produced  by the recurrent encoder change as we incrementally add more characters. For example,&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'c',
'co',
'com',
'comu',
'comua',
'comuan',
'comuanz',
'comuanza',
'comuanza ',
'comuanza g',
'comuanza gr',
'comuanza gre',
'comuanza gree',
'comuanza green'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'K',
'Ki',
'Kit',
'Kitc',
'Kitch',
'Kitche',
'Kitchen'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s start with a more qualitative approach, by obtaining the  intermediate feature vectors for each description and visualizing them in a two-dimensional space. Hopefully, we will be able to identify certain trajectories in vector space and their relationship to the associated colors.&lt;/p&gt;

&lt;p&gt;For example, for the color with the description &lt;strong&gt;&lt;em&gt;strawberry kiss&lt;/em&gt;&lt;/strong&gt;, we can obtain the vector from all its subsequences and reduce them altogether via PCA to generate the following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/strawberry_kiss.png&quot; alt=&quot;sk&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The associated transitions are presented in the following animation, where we can see that as we progress on the addition of characters, the distance between the vectors begin to decrease. This makes sense as at the beginning there is not restriction imposed by the conditional probabilities, but as we progress, the context associated to the previously seen characters narrows
the space of possible next characters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/sk5.gif&quot; alt=&quot;ska&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, the colors associated to each sub-sequence also converges. One thing to notice is, for example, in the word &lt;strong&gt;&lt;em&gt;strawberry&lt;/em&gt;&lt;/strong&gt;, when we get to the subsequence &lt;strong&gt;&lt;em&gt;straw&lt;/em&gt;&lt;/strong&gt;,  the color produced is between &lt;em&gt;light brown&lt;/em&gt;  and &lt;em&gt;pale yellow&lt;/em&gt;, which is characteristic to a &lt;em&gt;straw&lt;/em&gt; from  an agricultural perspective. As we continue adding characters, we can see how the color moves to a more red-ish spectrum, as &lt;strong&gt;&lt;em&gt;strawberry&lt;/em&gt;&lt;/strong&gt; is formed.  The same type of behavior can be seen in most of the descriptions.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;&lt;em&gt;Dijon mustard&lt;/em&gt;&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/dijon_mustard_ii.png&quot; alt=&quot;mustard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;&lt;em&gt;burnt blueberry&lt;/em&gt;&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/burnt_blueberry.png&quot; alt=&quot;burnt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A more interesting experiment is to visualize how two or more descriptions interact as their respective characters are appended. In that sense, let us assume a subset of descriptions that share a certain substring. As the generation is conditioned by the relative position of the characters, we can study the cases where the substring appears i) at the beginning of the descriptions (e.g. as &lt;strong&gt;&lt;em&gt;blue&lt;/em&gt;&lt;/strong&gt; in &lt;strong&gt;&lt;em&gt;blueberry&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;blue ocean&lt;/em&gt;&lt;/strong&gt;),  ii) in the middle or iii) at the end.&lt;/p&gt;

&lt;p&gt;For the  fist case, let us select the substring &lt;strong&gt;&lt;em&gt;gre&lt;/em&gt;&lt;/strong&gt; and sample ten descriptions from our test set the begin with such pattern:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'gre',
'greased lips',
'greasy spoon',
'great',
'great dark spot',
'great fright',
'great green',
'great lover',
'great party favors',
'great wide tea'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case, the resulting visualization is the following. 
(It must be noted that for the case of  two or more descriptions, the images generated by PCA are not expressive enough,
as most of the points are overlapped. For these cases, t-SNE performs much better.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/gre_set10.png&quot; alt=&quot;gre10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the corresponding animation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/gre5.gif&quot; alt=&quot;gre10ani&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the second case, if we select the substring &lt;strong&gt;&lt;em&gt;ran&lt;/em&gt;&lt;/strong&gt;, we obtain following sample set:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'Orange Creamsicle',
'arugula granita',
'orange scribbles',
'Light Amaranth',
'goranluppo',
'my random',
'orange celosia',
'Violet Fragrance',
'Pomegranate',
'red orange'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/ran.png&quot; alt=&quot;gre10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the last case, if we sample descriptions that end in &lt;strong&gt;&lt;em&gt;nge&lt;/em&gt;&lt;/strong&gt;, we obtain:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'Fringe',
'Lounge',
'Orange',
'avenge',
'change',
'fringe',
'lounge',
'orange',
'sponge',
'tounge'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/end_nge.png&quot; alt=&quot;end_nge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/end_nge_ani2.gif&quot; alt=&quot;end_nge_ani&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Visualizing the results, we can see the most interesting trajectories are generated when we define a substring  at the end, in the sense of obtaining trajectories that are more linear and encapsulated in certain zones of the vector space than in other cases.&lt;/p&gt;

&lt;p&gt;If you want to train your own model, the source for transferring from color names to LAB format can be found &lt;a href=&quot;https://github.com/pabloloyola/name-color&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, in this post we analyzed  the problem of producing a color given  a specific description of it. There are several extensions we can think of, but that is left to the reader’s imagination.&lt;/p&gt;

&lt;h3 id=&quot;from-colors-to-language&quot;&gt;From colors to language&lt;/h3&gt;

&lt;p&gt;Now we will try to do the inverse task, which means, give a color, expressed as a three-dimensional vector, obtain its description in natural language. To do that, we will take as inspiration  the approach described in &lt;a href=&quot;https://arxiv.org/abs/1606.03821&quot;&gt;Monroe et al.&lt;/a&gt; and try to obtain a working  version in Pytorch. The current implementation is available here: https://github.com/pabloloyola/color-description&lt;/p&gt;

&lt;p&gt;The data we will use in the first place is the dataset from &lt;a href=&quot;https://blog.xkcd.com/2010/05/03/color-survey-results/&quot;&gt;Munroe&lt;/a&gt;, which are the results of a survey where participants were asked to provide a short description of a given color. 
The actual version of the dataset is the one processed by &lt;a href=&quot;https://aclweb.org/anthology/Q15-1008&quot;&gt;McMahan et al.&lt;/a&gt; and that is available for download &lt;a href=&quot;http://paul.rutgers.edu/~bcm84/rugstk_v1.0.tar.gz&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The original representation of the colors is the HSV format, which stands for Hue, Saturation and Value. This format is intended to more closely resemble how humans perceive color attributes. Here is a small sample from the training set:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/munroe-data.png&quot; alt=&quot;munroe-data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the nature of the data collection, there are cases where one color description is associated to several –very similar– color vectors, and also cases  where one color vector is related to more than one color description.&lt;/p&gt;

&lt;p&gt;As described in the original paper, the model consists of an LSTM decoder that receives as input a color in the HSV format and the associated description, as a sequence of words.&lt;/p&gt;

&lt;p&gt;At each time step the LSTM is fed with a vector resulting from concatenating the vector representation of the color  we want to model, and the embedding vector associated to the word predicted by the model in the previous step. The output state from the LSTM is fed into a softmax layer from which the next word is selected.&lt;/p&gt;

&lt;p&gt;One interesting element to consider is  how to treat the color representation. We can just pass the raw three-dimensional vector as it is, assuming the the HSV space is sufficient to provide expressive representations. Another alternative is presented in the original paper, where it is proposed to transform the HSV vectors into a Fourier basis representation, resulting in  a 54-dimensional vector.&lt;/p&gt;

&lt;p&gt;An initial parameter search led to feasible results. Probably because the vocabulary size is not so big (as well as the sequences not so long). This is a random list showing a subset of the test set. The first column is the actual generated color description while the second and  third column represents that reference, in terms of the description and the color in RGB format.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/atomic.png&quot; alt=&quot;atomic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the model tends to output descriptions that while do not match exactly with the reference, they clearly resemble the target color. It is interesting to see the small perceptual differences between &lt;code class=&quot;highlighter-rouge&quot;&gt;orange&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;salmon&lt;/code&gt; and  &lt;code class=&quot;highlighter-rouge&quot;&gt;pink&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;magenta&lt;/code&gt;. 
One thing to notice in this case, is that the color descriptions are mostly formed by one word (maybe we should just call them color names in that case). This is a natural consequence of the nature of the dataset, where people were asked to be specific in their descriptions.&lt;/p&gt;

&lt;p&gt;If we filter a bit and collect results  where the generated sequences have length more than one (ignoring  &lt;code class=&quot;highlighter-rouge&quot;&gt;pad&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bos&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;eos&lt;/code&gt; special tokens), we can see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/not-atomic-blue.png&quot; alt=&quot;not-atomic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This a sample mostly from the &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt; spectrum, where can see two interesing things. Firstly, the accompanying words, usually adjectives, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;dark&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;light&lt;/code&gt;, are a bit repetitive, but effective. For example, in the third instance, we can see that the reference is just  &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt;, but if we look at the actual color, the generated description, &lt;code class=&quot;highlighter-rouge&quot;&gt;light blue&lt;/code&gt; seems to fit better. Of course,  we do not have a quantitative way to measure this as we do not have a point of reference of what is actually &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt;, but the model seems to learn globally such characteristics.&lt;/p&gt;

&lt;p&gt;If we filter a  bit more and consider only the generated descriptions and the references have more than one token, we obtain the following list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/2-2-green-blue.png&quot; alt=&quot;2-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, we see a more uniform matching between generated and reference descriptions.&lt;/p&gt;

&lt;p&gt;One interesting aspect to study is the impact of the Fourier transformation of the color vectors. The following graph compares the training and testing losses, showing that in fact expanding to a 54 dimensional representation improves model performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/losses_color_desc.png&quot; alt=&quot;losses_color_desc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Having explored the word-level approach, the natural next step is to
study if we can go a more fine grained level, for example, study if we can generate description character by character.
While this could represent an extra challenge for the model, I think it could also open the door to obtain more 
diverse descriptions. Let’s see.&lt;/p&gt;

&lt;p&gt;The most straightforward way to obtain a character level approach is to reuse the exsiting code and just change the tokenizer function to split the description into a sequence of characters.&lt;/p&gt;

&lt;p&gt;When I  was playing with the hyperparameters, something that caught my attention was how to structure the input vector for the decoder. As we saw above, this vector is the concatenation of the color representation (raw three-dimensional vector or an expansion based on the Fourier mapping) and the embbeded vector of the word predicted in the previous step.  For the word-based level, changing the proportion of the two components did not impact on the results. Something different I experienced in the character-level method. For example, the following figure compares the training and testing losses between three runs, using vectors of 50, 100 and 200 respectively. Again, the first column represents the generated description, and the second and third , the references.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/losses_embedding_size_character_level.png&quot; alt=&quot;losses_embedding_size_character_level&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Such difference also impact on the generated descriptions, for example:&lt;/p&gt;

&lt;p&gt;Sample of generated descriptions with embedded vector of size 50:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/size50.png&quot; alt=&quot;size50&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample of generated descriptions with embedded vector of size 100:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/size100.png&quot; alt=&quot;size100&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample of generated descriptions with embedded vector of size 200:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/size200.png&quot; alt=&quot;size200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the size of the character embedding indeed impacts on the generation results. A vector of size 50 seems to generate incomplete descriptions, meaning the it is not expressive enough to guide the generation to known descriptions.  A size of 200 totally makes the model collapse, generating the same pattern for the majority of the instances on the test set. Finally, a size of 100 seems to  provide much reliable results, in this case we can see the both the signal from the color and the embedded vector play nicely together, such as in the cases of &lt;code class=&quot;highlighter-rouge&quot;&gt;red&lt;/code&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;orange-brown&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;brown&lt;/code&gt;  for  &lt;code class=&quot;highlighter-rouge&quot;&gt;khaki&lt;/code&gt;, which means, while the model cannot always produce  the exact description, at least it is able to use in a effective way the color representation to find a feasible alternative.&lt;/p&gt;

&lt;p&gt;All the code and data generation scripts can be found here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://github.com/solargrammars/name_color&quot;&gt;https://github.com/solargrammars/name_color&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://github.com/solargrammars/color_descriptions&quot;&gt;https://github.com/solargrammars/color_descriptions&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Solar Grammars</name></author><category term="blog" /><summary type="html">Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based on that, contribute to understanding the principles of language acquisition. In that sense, color description, or more generally, the generation of color language, is a relevant aspect for understanding the emergence of human expression.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/learn-color-reps/gre_set10.png" /></entry></feed>