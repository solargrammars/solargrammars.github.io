<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-02-16T16:05:27+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Solar Grammars</title><subtitle></subtitle><author><name></name></author><entry><title type="html">Combining Word and Character level Representations from Color Descriptions</title><link href="http://localhost:4000/blog/color-gates.html" rel="alternate" type="text/html" title="Combining Word and Character level Representations from Color Descriptions" /><published>2017-01-04T00:00:00+09:00</published><updated>2017-01-04T00:00:00+09:00</updated><id>http://localhost:4000/blog/color-gates</id><content type="html" xml:base="http://localhost:4000/blog/color-gates.html">&lt;p&gt;We have explored the relationship between language and colors
from both word and character level perspectives, but in 
isolation. Then, the natural question that arises is what 
if subword structures play an important role in color 
grounding?  In other words, what if we combine character
and word level representations when modeling color-language
relationships?&lt;/p&gt;

&lt;p&gt;When we reviewed the results of the experiments on color generation
from descriptions using a character 
level recurrent model, we found quite interesting 
examples where, as we progressed on the sequences of 
characters, the generated colors suddenly changed. That was 
the case of descriptions such as &lt;code class=&quot;highlighter-rouge&quot;&gt;Strawberry lipstick&lt;/code&gt; (or 
something like that), where the color generation up the character
 &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; (from strawberry) was giving us a brown-ish color, naturally 
associated with the word &lt;code class=&quot;highlighter-rouge&quot;&gt;straw&lt;/code&gt;, which was also present during training. 
Then, suddenly, as the model incorporated the remaining tokens 
sequentially, &lt;code class=&quot;highlighter-rouge&quot;&gt;b -&amp;gt; e -&amp;gt; r -&amp;gt; r...&lt;/code&gt; the output color was 
gradually changing to something more reddish (associated to
strawberry).&lt;/p&gt;

&lt;p&gt;This led us to think, how does the grounding of a 
word or sentence can be understood in terms of the interplay
between its words and characters? For a given description, is it 
a specific character pattern that impact the most on the 
changes at color generation? or what are the common subword 
structures that command or condition more strongly the color 
generation?&lt;/p&gt;

&lt;p&gt;In order to answer those questions, we need a way to quantify 
the contribution of character and word level representations 
together. Lets say we have two ways to obtain a feature vector
for each word present in a description. The first one: we use a RNN  which learns a 
character based feature vector from the word, just like we did before.
The second one:
we directly query a source of pretrained vectors, such as Word2vec
or Glove. In this setting, which one contributes then most to minimize 
the loss, lets say, expressed as the MSE between the generated 
color and the ground truth? What type of combination is the optimal ?&lt;/p&gt;

&lt;p&gt;Fortunately, there has been quite interesting research
on the interplay between character and  word level 
representations (besides just concatenating both vectors) 
One way to represent such relationship is 
through the concept of &lt;code class=&quot;highlighter-rouge&quot;&gt;gates&lt;/code&gt;, such as in 
&lt;a href=&quot;https://arxiv.org/pdf/1904.05584.pdf&quot;&gt;Balazs et. al.&lt;/a&gt; 
or &lt;a href=&quot;https://arxiv.org/abs/1606.01700&quot;&gt;Miyamoto et. al.&lt;/a&gt;, which 
in simple terms means to learn a parameter $g$ that we can use 
to weight the vector representations coming from both char 
and word levels:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w = w_{char} * g +  w_{word} * (1-g)&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; can be a scalar or a vector. Both cases work almost 
the same, in which the idea is to have a fully connected layer 
that takes as input the word level representation and  outputs 
a scalar or a vector.&lt;/p&gt;

&lt;p&gt;Lets conduct a couple of experiments to see if this way of 
modeling color descriptions allow us to obtain interesting 
insights. As always, lets start with the data. For the 
purpose of this study, we can reuse the data
data extracted from ColourLovers, which we have used previously. 
This source of data is quite interesting given the variability
and abstractness in the use of language. Additionally, while 
they tend to be short (usually no more than five tokens) their
composition is quite diverse, as we can see if we extract
POS patterns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-gates-img/pos_freqs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lets obtain a sample from the top 5 groups of patterns with most 
descriptions. This is what we get:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;('ADJ', 'NOUN')&lt;/code&gt;:&lt;/p&gt;
&lt;table class=&quot;table_colors&quot;&gt;
&lt;tr&gt; &lt;td&gt;dutch teal&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#1a6470;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;hot pink&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ff67b4;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;mighty slate&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#556270;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;certain frogs&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#c3ff68;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;millennial blue&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#374bde;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;black tulip&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#340943;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;lipgloss boost&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#d7217e;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;green tea&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#edc003;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;cheery pink&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#fa3a7f;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;juicy pink&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#dd037e;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;('NOUN', 'NOUN')&lt;/code&gt;:&lt;/p&gt;
&lt;table class=&quot;table_colors&quot;&gt;
&lt;tr&gt; &lt;td&gt;vanilla cream&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#fbe6da;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;vitamin c&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#d26f36;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;orange icing&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ea3556;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;barbie world&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#e20092;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;sugar champagne&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#f9cdad;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;metro blue&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#2c89e6;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;brides blush&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ea69ae;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;moon warrior&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#51445f;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;love affair&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#856cc9;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;heirloom hydrangea&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#b954f2;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;

&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;('VERB', 'NOUN')&lt;/code&gt;:&lt;/p&gt;
&lt;table class=&quot;table_colors&quot;&gt;
&lt;tr&gt; &lt;td&gt;haunted milk&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#cdd7b6;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;raspberry lemonade&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#d2c2f1;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;minted peas&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#c8f0b2;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;strawberry lipgloss&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ce5d9b;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;irish sea&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#13a445;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;poached ivory&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#d8d8c0;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;wedded passion&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#630947;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;imagine roses&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#fe016b;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;rosen maiden&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ffa4a0;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;powdered sugar&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#bddebe;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Hmm it seems here  Spacy tagged as verbs things that are passive voice adjectives. It’s fine, as descriptions are short, probably 
Spacy didn’t have much context to work with when deciding the tag.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;('ADJ', 'NOUN', 'NOUN')&lt;/code&gt;:&lt;/p&gt;
&lt;table class=&quot;table_colors&quot;&gt;
&lt;tr&gt; &lt;td&gt;hawaiian lava fields&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#4f4e57;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;new yolk city&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ffa111;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;white chocolate fill&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#f8ecc9;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;unreal food pills&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#fa6900;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;red velvet cake&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#6e2e37;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;royal passion rc&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#4b1139;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;pure sun tanning&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#eb053f;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;soft lace pink&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ffebeb;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;sexy police officer&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#f7374b;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;cold morning dew&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#c8ebfe;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;

&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;('ADJ','ADJ','NOUN')&lt;/code&gt;:&lt;/p&gt;
&lt;table class=&quot;table_colors&quot;&gt;
&lt;tr&gt; &lt;td&gt;parttime super girl&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#e45635;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;expensive black wool&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#0f0a06;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;white kitten nose&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#ffd0d4;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;typical light grey&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#e7e7e7;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;old fine paper&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#f0eee1;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;rich red wedding&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#88090b;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;hawaiian blue light&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#66aef3;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;little blue eyes&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#486060;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;vintage new york&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#90797e;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;blue eyed girl&lt;/td&gt;&lt;td&gt;&lt;div style=&quot;float:left;width:100px; height:20px;background:#2b70bb;&quot;&gt;  &lt;/div&gt;  &lt;/td&gt;  &lt;/tr&gt;

&lt;/table&gt;

&lt;p&gt;In general we can see a good correlation between what each description 
expresses and the colors it has associated.&lt;/p&gt;

&lt;p&gt;There are several instances where the adjective is quite vague, 
such as in the case of &lt;code class=&quot;highlighter-rouge&quot;&gt;haunted&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;haunted milk&lt;/code&gt;. In those cases,
we can expect any model to have a hard time trying to figure
out how to learn the specific variation of the color. In that example,
the noun associated is &lt;code class=&quot;highlighter-rouge&quot;&gt;milk&lt;/code&gt;, which we know has the color &lt;code class=&quot;highlighter-rouge&quot;&gt;white&lt;/code&gt;
as the most probable reference. But then, how to turn a plain 
version of &lt;code class=&quot;highlighter-rouge&quot;&gt;white&lt;/code&gt; in to &lt;code class=&quot;highlighter-rouge&quot;&gt;haunted milk&lt;/code&gt;. Here is where it relies
my main hypothesis on the usefulness of the gating mechanism.
I expect models that are only character-based or only  word-based 
to perform poorly as it seems to be necessary  to consider 
a bit  more flexible representation of the description.&lt;/p&gt;

&lt;p&gt;For the &lt;code class=&quot;highlighter-rouge&quot;&gt;milk&lt;/code&gt; part of the description, I think , yes, just the 
word level vector might be enough. But for making sense of what is
&lt;code class=&quot;highlighter-rouge&quot;&gt;haunted&lt;/code&gt;, I think the model should take into account substructures such 
as &lt;code class=&quot;highlighter-rouge&quot;&gt;haunt&lt;/code&gt;, which can be only incorporated via the character level 
representation. Then, 
as &lt;code class=&quot;highlighter-rouge&quot;&gt;haunt&lt;/code&gt; can be more associated to &lt;code class=&quot;highlighter-rouge&quot;&gt;pale&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;ghostly&lt;/code&gt; terms (as in 
&lt;code class=&quot;highlighter-rouge&quot;&gt;The haunted house&lt;/code&gt;, or based on this interesting &lt;a href=&quot;https://www.thesaurus.com/browse/haunted&quot;&gt;list&lt;/a&gt; ), 
the model could learn to associate the colors from other
ghost-centric descriptions present in the data (thats important 
to highlight, I assume that &lt;code class=&quot;highlighter-rouge&quot;&gt;haunted&lt;/code&gt; in the sense ghost related stuff, 
but it could be the case the given the data, we are referring to 
another type of connotation. Anyway, &lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/haunt&quot;&gt;here&lt;/a&gt; 
is a good definition of what &lt;code class=&quot;highlighter-rouge&quot;&gt;haunt&lt;/code&gt; means).&lt;/p&gt;

&lt;p&gt;In summary, I think a gating mechanism could allow the model
to &lt;code class=&quot;highlighter-rouge&quot;&gt;learn to choose&lt;/code&gt; from which source (character level or word level)
when it needs to learn to  generate a color from a description. Moreover, 
the gates does not provide a binary value, but
a weight we can later inspect to give to obtain more explanatory insights.&lt;/p&gt;

&lt;p&gt;In terms of how to operationalize a gating mechanism, we can follow
the standard approach from the Balazs et. al. paper, as seen in the following diagram:
&lt;img src=&quot;/assets/img/blog/color-gates-img/gating-model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we have the description &lt;code class=&quot;highlighter-rouge&quot;&gt;vanilla cream&lt;/code&gt;. The ultimate
goal is to obtain a vector representation for the whole
description that can be fed into a linear layer to generate the 
color tuple (as a RGB or Lab color vector). We can do that by processing  each word at a time and 
then aggregate the word level features vectors. That can be done 
summing or averaging them, or, if we care about the ordering and
dependency between tokens (we should), we could use a word level
RNN over the word-level representations and use the final hidden vector
as a description-level vector.&lt;/p&gt;

&lt;p&gt;But the core part is how to obtain a word-level representation. Here 
is where the gating mechanism comes to play. For each word in the description, 
we follow previous work and run two simultaneous processes. The first one
consists of obtaining a character-level feature vector of the word by means
of a LSTM that runs over the sequence of characters. This representation accounts
for the morphology aspects of the word and allow us to give more 
flexibility to the final representation.&lt;/p&gt;

&lt;p&gt;The second process is to query
a lookup table and obtain a pretrained feature vector associated to the word,
for example Glove. This vector is used to compute the gate weight &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; 
by means of being fed directly to a linear layer. It is not direct that a 
pretrained source such as Glove could benefit this specific task. Given 
the nature of color description, and how words are associated in this context,
it could be the case that we could be actually sabotaging ourselves. 
In color descriptions, nouns serve usually as pivot from where an inherent color
is referenced. For example, when we describe a color as “sad banana” we know 
we are referring to a type of yellow. Same with something than contains nouns
such  as sun, school bus, vanilla, etc.&lt;/p&gt;

&lt;p&gt;Glove vectors were obtained through a task that is different to 
color description grounding, which is basically training a language model, where
we try to maximize the likelihood of a word given a defined context. In that sense,
if we assume two yellow things, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;banana&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;taxi&lt;/code&gt;, their vectors in Glove
will probably be quite distant, as those words do not co-occur so frequently. In 
Glove, the closest vectors to &lt;code class=&quot;highlighter-rouge&quot;&gt;banana&lt;/code&gt; are probably are associated to other fruits, 
such as apple or orange, and the closest vectors to &lt;code class=&quot;highlighter-rouge&quot;&gt;taxi&lt;/code&gt; will be probably 
associated to other transportation mediums. 
Glove is not color aware, or more generally, pretrained 
vectors are not attribute-aware. That does not mean we cannot use then as 
an initial representation and condition them based on additional information, such
as, for example, WordNet, or set them as trainable, which will naturally 
modify the spatial position. That could be an interesting experiment: to check how 
much the task changes de vectors we associate, in advance, to a defined color.&lt;/p&gt;

&lt;p&gt;Ok, having such model ready, and the data we discussed above, we can start our main
experiment. Lets consider the following models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Character-based LSTM (char-only)&lt;/li&gt;
  &lt;li&gt;Combine Character-level LSTM  and Word-level by concatenation
    &lt;ul&gt;
      &lt;li&gt;word level vectors initialized randomly (concat-random)&lt;/li&gt;
      &lt;li&gt;word level vectors initialized with Glove (concat-glove)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Combine Character-level LSTM  and Word-level by gating
    &lt;ul&gt;
      &lt;li&gt;word level vectors initialized randomly (gating-random)&lt;/li&gt;
      &lt;li&gt;word level vectors initialized with Glove (gating-glove)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s take a look at the results from this five models and see 
what interesting stuff we can get.&lt;/p&gt;

&lt;p&gt;Something important to notice is that models that combine
representations tend to be harder to train than just character 
level based model. In, in the exploratory runs, it was
kind of  difficult to find  a configuration that make 
the training stable. The most promising optimizer across
models was Adam, with a very low initial learning  of 0.0001. 
Overfitting was a huge issue, therefore using decay was essential
(more important that Dropout I would say )  to let the training and 
dev loss to achieve  a quasi-stable state.&lt;/p&gt;

&lt;p&gt;Regarding the use of pretrained vectors, does Glove help? 
Contrary to my initial assumption, it kinda help XD:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-gates-img/loss1clean.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above graph says a lot. In the first place we can see that 
when we use glove, in both glove and random models we can achieve
a lower dev loss. The difference is not that big but it is considerable
specially as both models reached a stable state in terms of their
training loss. The second interesting thing here is&lt;br /&gt;
that doe this dataset, basically gating does not give any 
advantage over the simple concatenation of word representations 
when we use glove, and when we use random vectors, clearly concatenation
beats gating by a respectable margin.&lt;/p&gt;

&lt;p&gt;Now, if we take the two best performing models and compare against
a char-only model, we can see a clear advantage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-gates-img/loss2clean.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The char-only model needs to learn from  scratch more fine-grained
relationships, therefore it is natural for it to be slower. Having 
access to a word level representation to encapsule characters as 
cohesive entities, seems to really speed up the training, and allows
to model to achieve a lower dev loss.&lt;/p&gt;

&lt;p&gt;Having visualized the losses, let’s take a look a the actual 
generated colors for a sample from  the unseen dataset.&lt;/p&gt;

&lt;table class=&quot;table_colors&quot;&gt;&lt;tr&gt;&lt;th&gt;Descripton&lt;/th&gt;&lt;th&gt;gating random&lt;/th&gt;&lt;th&gt;gating glove&lt;/th&gt;&lt;th&gt;concat random&lt;/th&gt;&lt;th&gt;concat glove&lt;/th&gt;&lt;th&gt;only char&lt;/th&gt;&lt;th&gt; Ground truth&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;bat ears&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#5b484a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#794946&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4f3f52&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#7e4945&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#989782&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8b7c83&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;sequoia glitter&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#90736a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8d6169&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#92776c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#936362&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#856a7a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#c46411&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;romantic music&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a47868&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a7667a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b0637a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ab687c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#927881&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#bbb9df&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;kitty espionage&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ac8372&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#958288&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b7926a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#958184&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8f787a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#fac162&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;first peach&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eea480&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f7a076&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f0a57f&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f6a67c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#e3ab7f&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eabc9d&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;sunny eyes&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b69582&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b5b36d&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b0b66c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#aab16e&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b2a271&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ff0579&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;black diamond eyes&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#434b59&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4a4c68&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#364567&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#233b5c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4e554a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#31161c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;empty canvas&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ab9e91&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#9aa292&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b1ad90&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#98a79a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a47e71&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#cedd90&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;tangerine fade&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ec7e55&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ea904b&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ed8f51&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eb8b51&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a28d76&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#efa04c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;witchy rendezvous&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#785a57&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#64405c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#825560&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#603255&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b08876&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#762075&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Well, the above experiment led us with a weird feeling. 
I was expecting that a gating mechanism really improved
the color generation over just a simple concatenation
of the word representations. But let’s not give up yet. 
Lets use a different scenario to 
to see if the gating mechanism can shine.&lt;/p&gt;

&lt;p&gt;In the previous experiment, for simplicity, we prepared the data
in a way that all words have an associated
pretrained vector (i.e., we discarded all the 
descriptions that contain at least one word
which do not appear in Glove). Thats a strong
assumption. In reality, in color description, as subjective as it is,
people can use any word they consider appropriate
to describe a color. Some words could be quite unique
or rare, tightly associated to the person’s cultural 
background or experiences. Most likely those words
will not have a pretrained vector available.&lt;/p&gt;

&lt;p&gt;Thats an interesting scenario to assess the usefulness 
of the gating mechanism. When a word we use has not 
an associated pretrained vector, what we basically do 
is to assign a random vector…which can be quite useless
in most cases.&lt;/p&gt;

&lt;p&gt;In that sense, we can see a good opportunity
for the gating mechanism: if we are currently trying 
to obtain a vector representation for a word in a sentence,
lets say the adjective &lt;code class=&quot;highlighter-rouge&quot;&gt;breakable&lt;/code&gt;
but such world does not have a pretrained vector associated, 
would it be great if the gating mechanism put more 
weight into the character level representation, which could
have possibly already learned good representations of words
that are associated to the target word, such as  &lt;code class=&quot;highlighter-rouge&quot;&gt;broken&lt;/code&gt; or
&lt;code class=&quot;highlighter-rouge&quot;&gt;breaking&lt;/code&gt; and their relationship with color generation.&lt;/p&gt;

&lt;p&gt;In order to test this new hypothesis, we need a new dataset
that actually contains words without a proper pretrained
representation and even noisier color descriptions. Lets 
generate such dataset and inspect its characteristics.&lt;/p&gt;

&lt;p&gt;Lets relax a little bit the descriptions we are allowing words
that do not appear initially in Glove, but still only
allowing words whose frequency is higher than two and also 
descriptions that are at least two word long. We can regard
this dataset as a bit noisier than the original one, but
not super noisy. If we repeat the experiment with
the same models, we obtain the following results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-gates-img/loss1medium.png&quot; alt=&quot;&quot; /&gt; 
&lt;img src=&quot;/assets/img/blog/color-gates-img/loss2medium.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table class=&quot;table_colors&quot;&gt;&lt;tr&gt;&lt;th&gt;Descripton&lt;/th&gt;&lt;th&gt;gating random&lt;/th&gt;&lt;th&gt;gating glove&lt;/th&gt;&lt;th&gt;concat random&lt;/th&gt;&lt;th&gt;concat glove&lt;/th&gt;&lt;th&gt;only char&lt;/th&gt;&lt;th&gt; Ground truth&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;bat ears&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#5b484a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#794946&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4f3f52&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#7e4945&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#989782&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8b7c83&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;sequoia glitter&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#90736a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8d6169&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#92776c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#936362&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#856a7a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#c46411&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;romantic music&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a47868&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a7667a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b0637a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ab687c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#927881&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#bbb9df&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;kitty espionage&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ac8372&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#958288&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b7926a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#958184&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8f787a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#fac162&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;first peach&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eea480&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f7a076&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f0a57f&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f6a67c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#e3ab7f&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eabc9d&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;sunny eyes&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b69582&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b5b36d&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b0b66c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#aab16e&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b2a271&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ff0579&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;black diamond eyes&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#434b59&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4a4c68&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#364567&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#233b5c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4e554a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#31161c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;empty canvas&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ab9e91&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#9aa292&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b1ad90&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#98a79a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a47e71&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#cedd90&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;tangerine fade&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ec7e55&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ea904b&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ed8f51&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eb8b51&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a28d76&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#efa04c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;witchy rendezvous&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#785a57&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#64405c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#825560&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#603255&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b08876&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#762075&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Hmmm, even with a noisier dataset, the results are quite
similar. No problem. Let’s force the model to use the 
raw dataset: straight out of the Colourlovers crawler, 
without any special filtering besides removing alphanumerical
symbols.&lt;/p&gt;

&lt;p&gt;Let us repeat the experiment with the same models, and now:
&lt;img src=&quot;/assets/img/blog/color-gates-img/loss1noise.png&quot; alt=&quot;&quot; /&gt; 
&lt;img src=&quot;/assets/img/blog/color-gates-img/loss2noise.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table class=&quot;table_colors&quot;&gt;&lt;tr&gt;&lt;th&gt;Descripton&lt;/th&gt;&lt;th&gt;gating random&lt;/th&gt;&lt;th&gt;gating glove&lt;/th&gt;&lt;th&gt;concat random&lt;/th&gt;&lt;th&gt;concat glove&lt;/th&gt;&lt;th&gt;only char&lt;/th&gt;&lt;th&gt; Ground truth&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;bat ears&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#5b484a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#794946&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4f3f52&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#7e4945&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#989782&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8b7c83&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;sequoia glitter&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#90736a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8d6169&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#92776c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#936362&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#856a7a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#c46411&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;romantic music&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a47868&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a7667a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b0637a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ab687c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#927881&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#bbb9df&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;kitty espionage&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ac8372&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#958288&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b7926a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#958184&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#8f787a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#fac162&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;first peach&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eea480&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f7a076&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f0a57f&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#f6a67c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#e3ab7f&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eabc9d&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;sunny eyes&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b69582&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b5b36d&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b0b66c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#aab16e&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b2a271&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ff0579&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;black diamond eyes&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#434b59&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4a4c68&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#364567&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#233b5c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#4e554a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#31161c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;empty canvas&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ab9e91&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#9aa292&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b1ad90&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#98a79a&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a47e71&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#cedd90&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;tangerine fade&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ec7e55&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ea904b&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#ed8f51&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#eb8b51&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#a28d76&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#efa04c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;witchy rendezvous&lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#785a57&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#64405c&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#825560&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#603255&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#b08876&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;td&gt; &lt;div style=&quot;width:60px; height:30px;background:#762075&quot;&gt; &lt;/div&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Here we can see that the gating model is able to reach
a lower test loss than the rest of alternatives.
But at the same time, we can see that the character-based
model is now able slightly outperform the other 
methods.&lt;/p&gt;

&lt;p&gt;Well, as a result, after taking into consideration the experiments and
the different datasets,  we cannot directly conclude that a gating mechanism 
always provide a better 
performance than the rest of configurations. We are considering a generic 
architecture for the specific problem of color grounding. My guess is
that we should design a more ad-hoc mechanism that takes into consideration
the unique characteristics of the problem. In the next post, lets keep 
exploring this problem and testing  some alternatives.&lt;/p&gt;</content><author><name>Solar Grammars</name></author><category term="blog" /><summary type="html">We have explored the relationship between language and colors from both word and character level perspectives, but in isolation. Then, the natural question that arises is what if subword structures play an important role in color grounding? In other words, what if we combine character and word level representations when modeling color-language relationships?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/arctic-1.jpg" /></entry><entry><title type="html">Palettes and Graphs as Color Context</title><link href="http://localhost:4000/blog/colors-in-context.html" rel="alternate" type="text/html" title="Palettes and Graphs as Color Context" /><published>2017-01-03T00:00:00+09:00</published><updated>2017-01-03T00:00:00+09:00</updated><id>http://localhost:4000/blog/colors-in-context</id><content type="html" xml:base="http://localhost:4000/blog/colors-in-context.html">&lt;p&gt;Colors are usually perceived in as part of a composition. It is rare that we can 
find them in isolation in nature. Sometimes we find
articles, usually associated to marketing, where people make a direct relationship
between a color and an emotion or a feeling. For example, when the color
red is associated to &lt;code class=&quot;highlighter-rouge&quot;&gt;passion&lt;/code&gt; and and green to &lt;code class=&quot;highlighter-rouge&quot;&gt;hope&lt;/code&gt;. Such relationships could
be true to some extent, but I think it is also important to consider the context associated
to such perceptions. A red color indeed can express passion, but in some cases, given 
the set of other colors present in the composition, it could also express sadness or fear.&lt;/p&gt;

&lt;p&gt;In that sense,  probably what colors evoke is 
tightly associated to the perception of a colorful &lt;code class=&quot;highlighter-rouge&quot;&gt;context&lt;/code&gt;. In other words, how we perceive 
a certain type of red, does not depend only on such color, but also on the colors
that surround it.&lt;/p&gt;

&lt;p&gt;As we are interested in learning feature representations of colors, we could
try to come up with a setting where colors appear as a group and then find associations
and, hopefully, patterns of co-occurrence between them.&lt;/p&gt;

&lt;p&gt;How can we find groups of colors we can work on? On simple way is to
collect a large amount of RGB images and  from each of them extract a color palette. A color
palette basically compresses an image into the most representative colors. We could say 
it is the most primitive way to do compression. While a palette cannot
provide a representation of the semantics of the image, at least it can inform us about
the main colors used and their distribution.&lt;/p&gt;

&lt;p&gt;Lets assume we have a set of images from which we can extract a color palette of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; colors, &lt;script type=&quot;math/tex&quot;&gt;p_i = \{ c_1, ..., c_n \}&lt;/script&gt;. 
With such dataset, we could conduct a simple experiment: We know that in natural language, 
we can assume that words that occur in the same contexts tend to have the similar meaning, i.e.,
&lt;em&gt;you can know a word by the company it keeps&lt;/em&gt;, which is known as the distributional hypothesis. 
Can we transfer such idea to colors, using the palettes as context?  In other words, 
can  we make the same analogy, and  say that colors that tend to co-occur  share certain
semantics? For example, let’s say we collect thousands of pictures of bathrooms. In most
of them &lt;code class=&quot;highlighter-rouge&quot;&gt;white&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;light blue&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pink&lt;/code&gt; will appear quite frequently, because there seems to be 
a common agreement on  how bathrooms look like, like a pattern in which such colors represent the
&lt;code class=&quot;highlighter-rouge&quot;&gt;cleanness&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;calmness&lt;/code&gt; a bathroom should have. Same if we analyze pictures of forests, probably
&lt;code class=&quot;highlighter-rouge&quot;&gt;greens&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;browns&lt;/code&gt; will be predominant, while &lt;code class=&quot;highlighter-rouge&quot;&gt;pinks&lt;/code&gt; will be very unlikely to appear. Can 
we leverage such regularities to learn dense feature vectors for each color?&lt;/p&gt;

&lt;p&gt;Lets start with a very simple approach, using very well known tools to see if we are heading
in a feasible direction. Firstly, we need some data.  For simplicity, lets use the validation set 
of the Microsoft COCO dataset. For each image, let’s capture a color palette of 6 colors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p1.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the above image, let’s compute a incremental palettes. As we can see, the colors are appended
in terms of their absolute proportion on the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p21.png&quot; style=&quot;height:50px;&quot; /&gt; &lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p22.png&quot; style=&quot;height:50px;&quot; /&gt; &lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p23.png&quot; style=&quot;height:50px;&quot; /&gt; &lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p24.png&quot; style=&quot;height:50px;&quot; /&gt; &lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p25.png&quot; style=&quot;height:50px;&quot; /&gt; &lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/p26.png&quot; style=&quot;height:50px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With this data, we can test a direct analogy from natural language BOW models.
Lets recall such formulation as , lets say we have a sentence &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; composed
by &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; words , &lt;script type=&quot;math/tex&quot;&gt;s  = \{  w_1, w_2 , ... , w_n \}&lt;/script&gt;, then a CBOW approach
learn vectors &lt;script type=&quot;math/tex&quot;&gt;v_i&lt;/script&gt; for a word &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; as maximizing the likelihood 
of &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; given its neighborhood  &lt;script type=&quot;math/tex&quot;&gt;\{ w_{i-c} , ... , w_{i+c} \}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;One problem we face at this moment is that each exact color probably appear only once
in one palette in all the dataset.  In other words, while we can
have two red colors that  look quite similar, in reality their RGB
representation will be different and therefore a model will perceive
them as different. To solve this issue, we can discretize their representation by clustering them.&lt;/p&gt;

&lt;p&gt;Then, for example, we can expect  all variations of &lt;code class=&quot;highlighter-rouge&quot;&gt;scarlet&lt;/code&gt; color to be grouped on a single 
cluster. Therefore, on a palette that contains one of these scarlet variations, instead of 
using the actual color, we can use the centroid/medoid of the associated cluster.&lt;/p&gt;

&lt;p&gt;Lets see first if clustering colors makes sense. For simplicity we can take the RGB or LAB 
tuples as feature vectors an apply K-means. In order to get a
rough estimation of the optimal number of clusters, we can take a look at both 
 distortion ( the average of the squared distances from the cluster centers of the respective 
 clusters ) and   inertia  (the sum of the squared distances to the closest cluster center). For 
 a sample of the data, we get the following: 
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/num_clusters_study2.png&quot; alt=&quot;num_clusters_study&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Which roughly tells us a good number of clusters is around 25. This makes sense as we are working 
with very standard natural images from COCO, so the colors they present should be quite compact. 
This result was obtained using a sample from 4000 images from which palettes of six colors were 
extracted, conforming a group of around 22.000 total colors that we clustered using their RGB 
representation. Changing  the amount of images or using the LAB format as feature vector, did 
not change considerably  the results.&lt;/p&gt;

&lt;p&gt;Lets take a look at the clusters visually. With &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; = 25, we can see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/0.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/1.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/2.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/3.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/4.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/5.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/6.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/7.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/8.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/9.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/10.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/11.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/12.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/13.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/14.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/15.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/16.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/17.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/18.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/19.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/20.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/21.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/22.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/23.png&quot; style=&quot;float:left;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/cluster_palettes/24.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we have 20 random elements from each cluster. As we can see, 
the groups tend to make sense, as there is a clear distinction between them and also a inherent cohesion within each cluster. Of course we 
can refine and adapt the number of clusters  based on the requirements of any  downstream task, but let’s use this result for the moment.&lt;/p&gt;

&lt;p&gt;With this, we can  associate each color with its cluster id so 
we can generate  the reduced palettes. Now, lets learn a vector representation from each element in the reduced
palettes, i.e., a dense vector for each cluster id. We want to exploit the co-occurrence of the colors in the palettes
making a direct analogy as how words co occur in a set of sentences. In that sense we can train the reduced palettes in
the same way sentences are used in CBOW:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/diagram2.png&quot; alt=&quot;diagram1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As stated above, the idea is to learn a feature vector for each color &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; as maximizing the likelihood 
of &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; given its neighborhood  &lt;script type=&quot;math/tex&quot;&gt;\{ c_{i-\lambda} , ... , c_{i+\lambda} \}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(c_i | \{ c_{i-\lambda} , ... , c_{i+\lambda} \} )&lt;/script&gt;

&lt;p&gt;CBOW serves as a nice inspiration, as there is no dependency between the elements in the context window. 
In our setting,  we also don’t want to enforce any ordering, as in the palette, we cannot assume the colors 
conform a sequence. Something interesting to notice is that when we obtain the original palettes, for each 
color we can obtain its proportion ( between 0 and 1) . This value
could be incorporated as a weight when we sum context vectors.&lt;/p&gt;

&lt;p&gt;One of the parameters that impacts the most on the performance of CBOW is actually the size of the context 
window. In NL, larger windows tend to favor more topical similarities, while shorter ones  privileges more 
functional similarities. In the context of this problem, we don’t know in advance such effect, therefore it 
will be necessary to explore empirically if there is any difference.&lt;/p&gt;

&lt;p&gt;For palettes of six colors, and considering 25 clusters,  moving the window context size from 2 to 5 really 
does not any noticeable impact. But when we start using  longer palettes, the differences are visible.
For example, here we show a small experiment with palettes of 20 colors and a number of clusters equal to 400. In 
the following figures, each learned representation is reduced to a two dimensional point space using TSNE . I
have associated to each point the original RGB color, so we can visualize the vectors in terms of their colorful nature.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_2.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_3.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_4.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_5.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_6.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_7.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_8.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_9.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/window_sizes/context_size_10.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Based on the above, we can start question ourselves a bit and 
try to ask what would make a good context-informed color
representation? Is it ok if we obtain vectors that group 
similar colors  into well defined groups? In some sense yes, 
but for such thing probably just using the RGB or LAB 
representation of the colors is enough. I think while it is
important  that the learned representations allow us to relate
similar colors, at the same time  I find it equally important that
these representation also encode the closeness of the colors 
in the context of the images the come from. Going back 
to the example we talked before, if we analyze images
of landscapes of forests, we probably can see groups of greens (lets
say , representing trees , vegetation ) closer to groups
of light blues and whites on one side (the clouds and sky)
and also some groups of brown and grays (the ground). 
It is not entirely clear which configuration is better, but
from the results, I would probably choose something between 4 and 5
for context window.&lt;/p&gt;

&lt;p&gt;At this point, we have a vector representation the can 
be associated to the whole cluster. Thats fine, but what we really
want is a vector for each color.  What we can do is to assume
that the learned vector is actually associated to the cluster
centroid (or medoid), then extrapolate the vector associated to 
each color in the cluster in terms of its distance to the centroid.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/diagram1.png&quot; alt=&quot;diagram1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above figure, lets say for cluster &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;  we obtained a vector representation
&lt;script type=&quot;math/tex&quot;&gt;v_k&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; be the medoid color and we arbitrarily associate &lt;script type=&quot;math/tex&quot;&gt;v_k&lt;/script&gt; to that point.
Then, we need to find a way to obtain a vector to all the other $c_j$ members of the cluster, taking
into account &lt;script type=&quot;math/tex&quot;&gt;v_k&lt;/script&gt; as reference. A natural way to proceed would be to use the distance  &lt;script type=&quot;math/tex&quot;&gt;d_{ij}&lt;/script&gt; between &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; and
&lt;script type=&quot;math/tex&quot;&gt;c_j&lt;/script&gt; as a way to weight &lt;script type=&quot;math/tex&quot;&gt;v_k&lt;/script&gt;, assuming that if the entities keep a certain relationship 
in the the RGB space, such relationship should be also present, to some extent,  in a learned 
feature space. Then, to produce a feature vector associated to &lt;script type=&quot;math/tex&quot;&gt;c_j&lt;/script&gt;, such representation
will have to keep certain direct proportion such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dist(c_i, c_j) \propto dist(v_k, v_j)&lt;/script&gt;

&lt;p&gt;Such  relationship probably is not linear as we are trying to relate  two dissimilar  feature spaces : the color one  (roughly assuming that RGB or LAB  spaces can be metric space … which strictly speaking is not correct), and the one defined by the feature representations from the co occurrence in the palettes. Finding  a factor that generalizes well across examples probably is hard. One way to proceed is parametrize such relationship via a small neural network. In that sense, we take all the pairs (centroid, vector)
and train a network. Then, at inference time, we pass all the real colors from to obtain a dense representation.&lt;/p&gt;

&lt;p&gt;Once we have obtained a vector representation for all colors present in our dataset, we can 
start visualizing them. The first thing we can do is to reduce the 300-dim vectors into two dimensions and plot them.  In the following figure, we can see each point to which we have associated its original color in the visualization.&lt;/p&gt;

&lt;p&gt;The first thing we can notice is the smooth transition across colors in most of the graph. Remember each point location is not
their RGB , but the learned dense vector. In that sense we can see that the greens seems to be quite close the 
certain variation of light brown/ orange, probably representing the co occurrence of landscapes. The same phenomena can be seen with the light blues and whites, whose closeness probably come from the disposition of those colors
in the sky. Interestingly, the fact that the blacks and browns appear in the center, means the that they tend to co occur with all other groups, which makes sense as in natural images darker colors are associated
with transversal elements such as shadows that appear across all images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/color_vectors_2d.png&quot; alt=&quot;color_vectors_2d&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/3d.png&quot; alt=&quot;color_vectors_2d&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;possible-improvements-optimizations&quot;&gt;Possible Improvements/ Optimizations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Single model:&lt;/em&gt; Our model is currently split into two main blocks. The fact the we have to perform a clustering
to reduce the palettes, while can be seen as a limitation, it also allows us to have more control, like a real pipeline. What
we can do  is try to merge the steps into one single model, by means of working directly with the original colors
from the palettes, without having to apply any reduction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Vector arithmetics:&lt;/em&gt; if we combine yellow and green in the RGB space,  we probably will obtain &lt;a href=&quot;https://youtu.be/0fC1qSxpmKo&quot;&gt;some kind of blue&lt;/a&gt;… most likely.  What if we combine the learned vectors associated to yellow and green? Will the resulting vector be similar to the learned vector of blue too?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Segmenting COCO or using other sources of images:&lt;/em&gt; COCO provides a set of very heterogeneous images: people, landscapes, sports etc. What if we run this pipeline on a subset of images that share the same semantics? Lets say, only on images of landscapes, or only images of restaurants. In those cases, for a given color, how the learned
representations obtained from different sets relate?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;incorporating-natural-language&quot;&gt;Incorporating Natural Language&lt;/h2&gt;

&lt;p&gt;So far, we have been considering only the images and their colors. But the COCO dataset
provides a set of captions for each image, a source of data that looks quite irresistible.
Then the question is, how can we incorporate such information? How the representations of the colors
could be &lt;code class=&quot;highlighter-rouge&quot;&gt;improved&lt;/code&gt; by informing our model/pipeline about the caption associated to the images we
use to obtain the palettes. I don’t know the answer yet, but lets explore a bit to see in which 
direction we should move.&lt;/p&gt;

&lt;p&gt;Lets not make things difficult and keep working with the validation part of COCO. For each image, we 
can obtain one or more captions.  The from each image &lt;script type=&quot;math/tex&quot;&gt;i \in I&lt;/script&gt;, we can obtain a 
pair &lt;script type=&quot;math/tex&quot;&gt;(p_i, s_i)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;p_i = \{ c_1, ..., c_N \}&lt;/script&gt; is  the associated palette of &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; colors, 
and &lt;script type=&quot;math/tex&quot;&gt;s_i = \{  w_1, ..., w_M \}&lt;/script&gt; the caption expressed as a sequence of  words. For each 
of these captions we can use GLOVE (or any other source of pretrained vectors) to
obtain  a good dense representation for each word.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/diagram3.png&quot; alt=&quot;diagram3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given this extra source of information, how could we use the co occurrence patterns associated to 
the words to support the context based representation learning of colors? We know there is a one 
to one relationship between the sentences and the palettes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/diagram4.png&quot; alt=&quot;diagram4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hmmm it seems there is going to be necessary to obtain a representation of the sentence, &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;. That 
is not difficult, as we can just do a plain average of the word vectors in &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;, use something 
like &lt;a href=&quot;http://proceedings.mlr.press/v32/le14.pdf&quot;&gt;Doc2vec&lt;/a&gt; or if we care about the dependencies (we probably should), 
we can use an RNN to encode the sentence and  capture the last hidden state as a representation 
of the sentence.  In any case, lets assume that for each sentence &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; we obtain a dense sentence vector &lt;script type=&quot;math/tex&quot;&gt;sv_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Lets start with a very simple experiment. For each pair &lt;script type=&quot;math/tex&quot;&gt;(p_i, sv_i)&lt;/script&gt; lets use kNN to obtain the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 
closest pairs, based on the cosine similarity between sentence vectors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_i, sv_i    \rightarrow kNN_{sv_i} = \{  sv_1, sv_2 , ... , sv_K\}&lt;/script&gt;

&lt;p&gt;Each of these sentence vectors have in turn associated a palette. Therefore, we can , 
transitively, associate the palette &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; with a set of &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; palettes based on the 
underlying similarity of their associated sentences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/diagram6.png&quot; alt=&quot;diagram6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, for the following  instance:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a man in mid air attempting to catch a frisbee&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/palette-caption-q.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;the closest five pairs are:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a man reaching out to catch a frisbee&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc1.png&quot; alt=&quot;pcq&quot; /&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;a man jumps to catch a frisbee flying through the air&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc2.png&quot; alt=&quot;pcq&quot; /&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;there is a dog in the air going to catch a frisbee&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc3.png&quot; alt=&quot;pcq&quot; /&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;a man in a grassy field about to catch a frisbee&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc4.png&quot; alt=&quot;pcq&quot; /&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;a man is in mid air doing a skateboard trick&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc5.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that while simple, the sentence embedding is able 
to capture quite relevant semantic relationships between sentences. Of course,
it degrades quite quickly, but that a is characteristic of the data under 
study.&lt;/p&gt;

&lt;p&gt;More interesting is the relationship between the colors. At first glance
I cannot see any direct similarity at palette level on this example. Actually, it looks
pretty random. I think this is because for this sample, the 
core word is &lt;code class=&quot;highlighter-rouge&quot;&gt;frisbee&lt;/code&gt;, which inherently does not have an associated color, right?
What if we sample the captions by a word that can be easily associated to
a color, for example &lt;strong&gt;forest&lt;/strong&gt;, which I expected to be linked to greens.&lt;/p&gt;

&lt;p&gt;For a given sample containing the &lt;strong&gt;forest&lt;/strong&gt; word:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a bench in the middle of a lush green forest&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc6.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We obtain the following five closest:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a tree sitting in the middle of a lush green field&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc7.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a road in the middle of a beautiful green forest&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc8.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a large brown cow standing on the side of a lush green hill&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc9.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a man and child standing on top of a lush green field&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc10.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a photo of a bench in the middle of the beach&lt;/code&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/pc11.png&quot; alt=&quot;pcq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there is still quick degradation on the semantic 
similarity across sentences, at least we can see a bit
more cohesion at color level.&lt;/p&gt;

&lt;p&gt;This is getting more interesting. We could then  TODO&lt;/p&gt;

&lt;h2 id=&quot;more-structure-through-graphs&quot;&gt;More Structure through graphs&lt;/h2&gt;

&lt;p&gt;We have explored the feature learning  of colors by means of
capturing their co-occurrence using palettes. While palettes are 
easy to obtain, they do not encode any kind of structural information. 
In contrast, if we rely on graph we could have a richer setting
that could allow a more comprehensive learning.&lt;/p&gt;

&lt;p&gt;How could we extract a color graph from an image? Actually, there 
are families of techniques from image processing that compute an
intermediate graph between image segments, for example, when 
performing image compression.&lt;/p&gt;

&lt;p&gt;Lets see some examples of color graphs obtained from both COCO dataset
samples as well from art pieces from the WikiArt website:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;COCO&lt;/code&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco1.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco1_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco2.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco2_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco3.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco3_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco4.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco4_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco5.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/coco5_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WIKIART&lt;/code&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki1.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki1_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki2.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki2_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki3.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki3_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki4.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki4_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki5.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/image_graphs/wiki5_graph.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We can see that the resulting graphs represent a good discretization 
of the images, as they seems to cover the color spectrum  quite well.
The nodes position are not aligned with the actual image 
composition, but that can be added.&lt;/p&gt;

&lt;p&gt;Having computed these graphs for a set of images, how can we learn
feature representations that encode the relationships between colors?
In other words, not only learn a dense vector for each node, but also 
for the edges that connect them.&lt;/p&gt;

&lt;p&gt;One easy way would be to reuse most of the code we wrote before and 
re-apply a CBOW-like approach by sampling random walks from the graph
This is the idea behind existing approaches such  as &lt;a href=&quot;https://arxiv.org/abs/1607.00653&quot;&gt;Node2vec&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/node2vec.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One nice thing about this idea is that the random walks could allow us
to visualize the progression of colors in certain zones. But at the same time,
as we need to define in advance a window, that could be restricting 
long term color dependencies present on the images.&lt;/p&gt;

&lt;p&gt;Lets explore another alternative that is provided  by the family of 
graph neural networks. This family of models are based on a 
message passing schema, where we iteratively learn node representations
by propagating information through the edge structure. In that sense,
the representation for a given node at a given time is the result 
of the aggregation function on all its neighbor nodes.&lt;/p&gt;

&lt;p&gt;Recently, there has been an extensive list of publications, 
each of them proposing a specific variation in terms of the how the
representations are computed. A paper by &lt;a href=&quot;https://arxiv.org/abs/1704.01212&quot;&gt;Gilmer et. al.&lt;/a&gt; 
does a great job (basically showing 
that the myriads of papers that have appeared in the last years are … basically
the same) by
summarizing all the literature and providing a description of the core set of 
functionalities underneath the learning process. In short, if we assume
an undirected graph  &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;, node features &lt;script type=&quot;math/tex&quot;&gt;x_v&lt;/script&gt; and edge features &lt;script type=&quot;math/tex&quot;&gt;e_{vw}&lt;/script&gt;, 
we will have a message passing phase that runs for &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; time steps, 
defined in terms of a message function &lt;script type=&quot;math/tex&quot;&gt;M_t&lt;/script&gt; and a vertex update function &lt;script type=&quot;math/tex&quot;&gt;U_t&lt;/script&gt;. 
Hidden states &lt;script type=&quot;math/tex&quot;&gt;h_v^t&lt;/script&gt; at each node are updated based on messages &lt;script type=&quot;math/tex&quot;&gt;m_v^{t+1}&lt;/script&gt;,
which in turn is defined as &lt;script type=&quot;math/tex&quot;&gt;m_v^{t+1} = \sum_{w \in N(v)}M_t(h_v^t,h_w^t, e_{vw})&lt;/script&gt;
and with  &lt;script type=&quot;math/tex&quot;&gt;h_v^{t+1} = U_t(h_v^t,m_v^{t+1})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;A subsequent readout phase can compute feature vector for the whole graph, using readout, 
function &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\hat{y} = R(\{ h_v^T|v \in G  \})&lt;/script&gt;. 
Here, as you can image, &lt;script type=&quot;math/tex&quot;&gt;M_t, U_t, R&lt;/script&gt; are learnable functions.&lt;/p&gt;

&lt;p&gt;In our case, we can easily use this framework in a semi supervised learning setting.
For each graph associated to each image, we can let the model know the labels (RGB values)
only from a portion of the nodes. Then, the goal is to infer the RGB vector
associated to  each node, including both labeled and unlabeled.  The assumption
behind this is that the message passing mechanism will eventually be able to 
propagate information from the seen to the unseen nodes, taking into account 
the locality of the color composition expressed by the edge structure. This is an important
point to notice: colors in a composition such a natural image tend to form
neighborhoods, and they  can be as granular as we want, depending on the parameters
chosen when we compress the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/message_passing.png&quot; width=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above figure, we expect the unlabeled node, represented in white, iteratively
receives a color signal from its neighborhood in the form of a node vector, 
which can be used to learn the correct RGB (or Lab) tuple.&lt;/p&gt;

&lt;p&gt;Something noticed during the exploratory training is how the proportion of 
labeled nodes impacts on the performance of the color generation. Naturally, the more
nodes we have labeled, the better the results, as more information will be encoded in 
the graph. Here we can see how a small portion of nodes compare against a big one. 
While in terms of loss in the labeled set there no much difference, it is clear 
that  in the case of the unlabeled nodes, there is a advantage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/colors-in-context-img/losses3.png&quot; /&gt;
&lt;img src=&quot;/assets/img/blog/colors-in-context-img/losses4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Just the number of nodes is not enough, but its connectivity
is also very important. As we are in a message passing configuration, if the information
cannot be propagated from the labeled nodes to the unlabeled ones, the learning will 
be poor even if we consider a larger proportion of nodes with labels. In that sense,
average degree of the nodes we have adding is more relevant than the pure number of them. 
That is an interesting  design decision in terms of data acquisition when designing 
a machine learning model under this scenario&lt;/p&gt;

&lt;p&gt;While the loss can help us to automate the the search for better parameters, does not
tell much about the actual results. Lets take a look at some samples from the generated
colors. In the first place, we have the results for the labeled set. It seems to
be a easy task for the model, naturally. This is just a safe check, just to 
make sure we are not making any big mistake. In any case, I can see clear differences in terms of
brightness, specially in the case of pinks.&lt;/p&gt;

&lt;table class=&quot;table_colors&quot; style=&quot;width:500px;&quot;&gt;
&lt;tr&gt;&lt;td&gt; Generated &lt;/td&gt; &lt;td&gt;  Ground truth &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#7c9e7a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#5f9177&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#732d83&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#8e4187&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#c7767c&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#9e7879&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#c18076&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#3e193c&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#3e0847&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#ce7c8a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#7d80dd&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#9784d0&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#5a8f8b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#46968b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#e0609d&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#c5757f&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#9e7879&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Now, the more interesting result. Here we can see a sample of of the color generation for nodes that
from the unlabeled set, i.e., the produced color is the result of the label propagation through the link structure
of the graphs under study. The following  sample is obtained randomly, and it is presented sorted in terms
of the euclidean distance between  the generated and ground truth color, so we can visualize more effectively how the 
results degrade.&lt;/p&gt;

&lt;table class=&quot;table_colors&quot; style=&quot;width:500px;&quot;&gt;
&lt;tr&gt;&lt;td&gt; Generated &lt;/td&gt; &lt;td&gt;  Ground truth &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#499b93&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#46968b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#e86d9e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#5a8f81&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#46968b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#db7c9b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#72278c&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#8e4187&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#846a57&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#c78776&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#b2bd5e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#dab872&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#c08775&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#df678a&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#e3dde1&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#fffffe&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#e7f074&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#fefe44&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#e4de34&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#aae252&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#60cbb2&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#46968b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#612a7d&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#4a3d3b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#9d19c4&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#8e4187&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#861a36&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#b94859&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#73e171&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#5f9177&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#3eb87f&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#76932e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#58e730&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#a4d66b&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#aa9522&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#fe7b4f&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#56661d&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#59621c&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#206125&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#5d9670&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#e6dfcd&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#a6948f&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#9feaa2&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#808178&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#501d41&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#eae9cd&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#a6948f&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#1a1f3e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#7e7d7e&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; &lt;div style=&quot;width:200px; height:20px;background:#551b28&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;  &lt;div style=&quot;width:200px; height:20px;background:#fe0a01&quot;&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;While there are clear mistakes by the model, specially at the end of the list, I think in general the these is a feasible correspondence. This means,
the node representation we are learning via the graph neural net is able to encode in a good way the the information propagated by the neighboring nodes, 
in this case, the color. There are several things we can discuss from this small experiment and  its limitations.&lt;/p&gt;

&lt;p&gt;For example, when we generate the graphs,  we first discretize the image into color zones, given a specified threshold. This resulting set of colors will be our nodes.
Then, as mentioned above, the edges are generated in terms of if two color zones  are contiguous. In that sense, for most parts , edges associate two colors
that are similar, or belong to the same portion of the color spectrum. But naturally there are cases were two color zones are contiguous, but their colors are totally
different. Think , for example, in the case of an image that has grass next to a road. In this case, the is a big chance that a green gets linked a grey. Having
this type of edges is part of the nature of the problem, but certainly  could be confusing the model, as one unlabeled node could be receiving  information 
that is two heterogeneous, that ultimately its generated color will be kind of random. An alternative could to just ignore edges between nodes that are too dissimilar,
but in turn that could generate a very disjoint graph, just small connected components without any inter-color transference. That is not so interesting.&lt;/p&gt;

&lt;p&gt;The code to reproduce the experiments on this article will be available here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/solargrammars/colors-in-context&quot;&gt;https://github.com/solargrammars/colors-in-context&lt;/a&gt;&lt;/p&gt;</content><author><name>Solar Grammars</name></author><category term="blog" /><summary type="html">Colors are usually perceived in as part of a composition. It is rare that we can find them in isolation in nature. Sometimes we find articles, usually associated to marketing, where people make a direct relationship between a color and an emotion or a feeling. For example, when the color red is associated to passion and and green to hope. Such relationships could be true to some extent, but I think it is also important to consider the context associated to such perceptions. A red color indeed can express passion, but in some cases, given the set of other colors present in the composition, it could also express sadness or fear.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/arctic-1.jpg" /></entry><entry><title type="html">Grounding Color Comparatives</title><link href="http://localhost:4000/blog/grounding-color-comparatives.html" rel="alternate" type="text/html" title="Grounding Color Comparatives" /><published>2017-01-02T00:00:00+09:00</published><updated>2017-01-02T00:00:00+09:00</updated><id>http://localhost:4000/blog/grounding-color-comparatives</id><content type="html" xml:base="http://localhost:4000/blog/grounding-color-comparatives.html">&lt;p&gt;The methods  we have covered so far allowed  us to learn  a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when
we say a given color is &lt;code class=&quot;highlighter-rouge&quot;&gt;mustard yellow&lt;/code&gt;, we are basically relating our experienced notion of &lt;code class=&quot;highlighter-rouge&quot;&gt;mustard&lt;/code&gt;  and a reference &lt;code class=&quot;highlighter-rouge&quot;&gt;yellow&lt;/code&gt; and associating them
to express our perception through language. Another example can be seen when we say &lt;code class=&quot;highlighter-rouge&quot;&gt;dark green&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;deep blue&lt;/code&gt;. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of  &lt;code class=&quot;highlighter-rouge&quot;&gt;dark green&lt;/code&gt;, we use our preconceived reference of what we understand as &lt;code class=&quot;highlighter-rouge&quot;&gt;green&lt;/code&gt; and its &lt;code class=&quot;highlighter-rouge&quot;&gt;dark&lt;/code&gt; variant. In this process, we are implicitly internalizing that the color we are perceiving is &lt;code class=&quot;highlighter-rouge&quot;&gt;darker&lt;/code&gt; than 
the reference we have.&lt;/p&gt;

&lt;p&gt;It could be interesting to model or quantify the distance between a predefined reference and a newly perceived color, and how such distance condition the language we use to  describe it.&lt;/p&gt;

&lt;p&gt;In an &lt;a href=&quot;https://www.aclweb.org/anthology/P18-2125&quot;&gt;ACL’18 paper&lt;/a&gt;, Winn et al. proposed a way to ground color comparatives as vectors in a color space. Let’s assume we have a reference color &lt;code class=&quot;highlighter-rouge&quot;&gt;yellow&lt;/code&gt; and a given target color &lt;code class=&quot;highlighter-rouge&quot;&gt;dark yellow&lt;/code&gt;, both expressed as RGB points. The associated comparative in this case is &lt;code class=&quot;highlighter-rouge&quot;&gt;darker&lt;/code&gt;, as naturally, &lt;code class=&quot;highlighter-rouge&quot;&gt;dark yellow&lt;/code&gt;
 is &lt;em&gt;darker&lt;/em&gt; than &lt;code class=&quot;highlighter-rouge&quot;&gt;yellow&lt;/code&gt; :). The key idea is to represent a comparative between both colors as a vector rooted at the reference  and that points in the direction of the the target.&lt;/p&gt;

&lt;p&gt;The first interesting thing about that work is the construction of an ad-hoc dataset. The authors take as starting point  the dataset from &lt;a href=&quot;https://www.aclweb.org/anthology/Q15-1008&quot;&gt;McMahan et al.&lt;/a&gt; (which in turn is a processed version of the original &lt;a href=&quot;https://blog.xkcd.com/2010/05/03/color-survey-results/&quot;&gt;XKCD color survey&lt;/a&gt;). This dataset associates a color description with a RGB point.&lt;/p&gt;

&lt;p&gt;The authors of this paper noticed that a considerable amount of descriptions contain
adjectives, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;light green&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;dark purple&lt;/code&gt;. From those adjectives, they obtained the associated comparatives (  &lt;code class=&quot;highlighter-rouge&quot;&gt;light&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;lighter&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;dark&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;darker&lt;/code&gt;) so they can
conform tuples like  &lt;code class=&quot;highlighter-rouge&quot;&gt;(green, darker, dark green)&lt;/code&gt;, where 
the first element represent the reference color as RGB tuple, the second is the associated comparative, and third element is the target color, also expressed as a  RGB tuple.&lt;/p&gt;

&lt;p&gt;Let’s take a look a some tuples and their associated points, by plotting a two dimensional reduction via T-SNE. For example, in the following figure we can see how the RGB points for &lt;code class=&quot;highlighter-rouge&quot;&gt;lavender&lt;/code&gt; are positioned  in relation to its available comparatives.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_lavender.png&quot; alt=&quot;lavender&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing we can immediately notice is that … some does not quite resemble the common notion of &lt;code class=&quot;highlighter-rouge&quot;&gt;lavender&lt;/code&gt;. But that is an interesting aspect of the dataset, as such categorization comes from a real survey, i.e., it compresses the diversity of perception from a huge number of people.  I added a marker associated to the mean value of comparative (&lt;code class=&quot;highlighter-rouge&quot;&gt;average&lt;/code&gt; in this case means just plain &lt;code class=&quot;highlighter-rouge&quot;&gt;lavender&lt;/code&gt;) and we can easy distinguish the defined color zones. Interestingly, the &lt;code class=&quot;highlighter-rouge&quot;&gt;light lavender&lt;/code&gt; seems to be quite different from the rest, but between elements in this group, we still can see distances quite considerable. If we take a look at a couple more of examples,  we see a consistent distinction:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rose&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_rose.png&quot; alt=&quot;mint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;turquoise&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_turquoise.png&quot; alt=&quot;mint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;violet&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/visual_analysis_violet.png&quot; alt=&quot;peach&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Something worth noticing there could be several ways to structure  a training set. In the case of the paper from Winn et al.,  instead of forming all possible reference-target pairs between RGB points, they assume that the references converge, therefore only the average values of those sets are used.&lt;/p&gt;

&lt;p&gt;The model consists of a linear layer that receives the concatenation of the  reference color &lt;script type=&quot;math/tex&quot;&gt;r_c&lt;/script&gt; (as RGB three-dimensional vector) and an embedding associated to the comparative &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; (in the paper the authors used the  Google’s 300-dimensional word2vec pretrained vectors). The resulting representation is fed to an additional layer along with the reference color and the output consists of a 3 dimensional vector &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; that represents the direction in the color space of the vector rooted at &lt;script type=&quot;math/tex&quot;&gt;r_c&lt;/script&gt; and the &lt;em&gt;follows&lt;/em&gt; the direction towards the target &lt;script type=&quot;math/tex&quot;&gt;t_c&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The loss function has two components, the first one computes the cosine distance between the learned vector &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; and the vector that connects the reference and the target colors ( &lt;script type=&quot;math/tex&quot;&gt;\vec{tr} =  t_c - r_c&lt;/script&gt;).
The second component compares the size of the learned vector against the size of the vector associated to the target color. With this, it is expected to modulate  the distance to the target color so that the colors in the line of the vector resemble the comparative.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some examples of learned comparative vectors.  In the case of the tuple &lt;code class=&quot;highlighter-rouge&quot;&gt;(lavender, darker, dark lavender)&lt;/code&gt; the first figure shows the differences between the &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\vec{tr}&lt;/script&gt; as we graph them in a 
three dimensional space. As RGB is not a well defined metric space, the colors were transformed into the LAB
space. The second figure shows the the color gradient associated to the &lt;script type=&quot;math/tex&quot;&gt;\vec{tr}&lt;/script&gt; vector and the one associated to &lt;script type=&quot;math/tex&quot;&gt;\vec{v_g}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/vector_visualization_darklavender.png&quot; alt=&quot;vvis&quot; /&gt;  &lt;img src=&quot;/assets/img/blog/color-comparatives/color_gradients_darklavender.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the figures above, we can see that the model actually does a good job, as &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; follows 
quite close the direction of &lt;script type=&quot;math/tex&quot;&gt;\vec{tr}&lt;/script&gt;. In terms of magnitude, &lt;script type=&quot;math/tex&quot;&gt;\vec{wg}&lt;/script&gt; is longer, which may say something
about the need to adjust the second term of the loss function. But in practical terms, if we look at the color
gradients, we can see that &lt;script type=&quot;math/tex&quot;&gt;\vec{wg}&lt;/script&gt; also takes us to a state that we could describe as  &lt;code class=&quot;highlighter-rouge&quot;&gt;dark lavender&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If we inspect other pairs, we can see the a similar behavior.&lt;/p&gt;

&lt;p&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;(yellow, more orange, orange yellow)&lt;/code&gt;, we see a quite close fit in terms of the direction, while at the same time a bigger difference 
in term of the magnitudes. This disagreement seems to not make much difference regarding the colors we can obtain. 
&lt;img src=&quot;/assets/img/blog/color-comparatives/vector_visualization_orangeyellow.png&quot; alt=&quot;vvis&quot; /&gt;  &lt;img src=&quot;/assets/img/blog/color-comparatives/color_gradients_orangeyellow.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;(mint, darker, dark mint)&lt;/code&gt;:
&lt;img src=&quot;/assets/img/blog/color-comparatives/vector_visualization_darkmint.png&quot; alt=&quot;vvis&quot; /&gt;  &lt;img src=&quot;/assets/img/blog/color-comparatives/color_gradients_darkmint.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In general,  the method works quite well for tuples on the test set whose elements have appeared during training, which means
RGB  tuples that are in the proximity of colors already seen. Naturally, things get more interesting when we pass a tuple in which  one or more components are totally out of distribution.&lt;/p&gt;

&lt;p&gt;For example, lets take a look at the tuple &lt;code class=&quot;highlighter-rouge&quot;&gt;(pale blue, more , very pale blue)&lt;/code&gt;. In this case &lt;code class=&quot;highlighter-rouge&quot;&gt;pale blue&lt;/code&gt; was not present 
on the training data. If we compute the color gradients using 
the grounded comparative vector, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/unseen_ref_color_gradients_verypaleblue.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see a clear difference on the ending color both gradients
reach, with the one associated to &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt; getting closer to what we could consider &lt;em&gt;muddier&lt;/em&gt; rather than &lt;em&gt;very pale&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Another setting we can explore is when the comparative was not seen during training. For example,  for the tuple &lt;code class=&quot;highlighter-rouge&quot;&gt;(pink, purplish, purplish pink)&lt;/code&gt;, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/color-comparatives/unseen_comp_color_gradients_purplishpink.png&quot; alt=&quot;vvis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we can see how the model is having a really hard time
trying to align we the ground truth. In fact,  we can see the 
ending colors do not resemble at all, and  in the case of
the color gradient associated to &lt;script type=&quot;math/tex&quot;&gt;\vec{w_g}&lt;/script&gt;, there is no sign 
of  &lt;em&gt;purplishness&lt;/em&gt; :)&lt;/p&gt;

&lt;p&gt;This tells us a lot about how important is for the model
to handle the comparative representations. As we reviewed above, in the original paper the vectors associated to the comparative
tokens are pretrained vectors from Google Word2vec. Of course replacing those vectors with other, more refined, pretrainied sources could allow more generalization, but I wonder if there is any way to consider out of vocabulary comparatives in a more comprehensive way.&lt;/p&gt;

&lt;p&gt;One way to approach such problem could be to estimate the direction of an unseen comparative based on the semantic relationship it may  have with existing comparatives. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;brighter&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;lighter&lt;/code&gt; , while not pointing in the exact same direction, probably are close enough. Therefore, if one of them  not appear, probably their RGB points could 
make the model be aware of their relationship, guiding it towards a feasible region in the  color space. Unfortunately, such relationship is in directly encoded in the pretrained vectors so it might be necessary to induce such bias. Besides that, I think the most interesting extension could be 
to incorporate structure  (hierarchies ? ) to the comparative relationships, maybe based on an external source. But lets keep
that as a future work for the reader.&lt;/p&gt;

&lt;p&gt;All the code, data generation and  figures can be found here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://github.com/solargrammars/comparative_color_descriptions&quot;&gt;http://github.com/solargrammars/comparative_color_descriptions&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Solar Grammars</name></author><category term="blog" /><summary type="html">The methods we have covered so far allowed us to learn a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when we say a given color is mustard yellow, we are basically relating our experienced notion of mustard and a reference yellow and associating them to express our perception through language. Another example can be seen when we say dark green or deep blue. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of dark green, we use our preconceived reference of what we understand as green and its dark variant. In this process, we are implicitly internalizing that the color we are perceiving is darker than the reference we have.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/arctic-1.jpg" /></entry><entry><title type="html">Learning Color-Language Representations</title><link href="http://localhost:4000/blog/learning-color-language-representations.html" rel="alternate" type="text/html" title="Learning Color-Language Representations" /><published>2017-01-01T00:00:00+09:00</published><updated>2017-01-01T00:00:00+09:00</updated><id>http://localhost:4000/blog/learning-color-language-representations</id><content type="html" xml:base="http://localhost:4000/blog/learning-color-language-representations.html">&lt;p&gt;Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based 
on that, contribute to understanding the principles of language 
acquisition. In that sense, color description, or more generally,  the generation of color language, is a relevant aspect for  understanding the emergence of human expression.&lt;/p&gt;

&lt;p&gt;The use of colors as constructs to describe situations can be seen 
in the literature. Some uses are intriguing, for example&lt;br /&gt;
William Gibson’s Neuromancer begins with “&lt;em&gt;The sky above the 
port was the color of television, turned to a dead channel&lt;/em&gt;”. What 
exactly did the author want to express here? Of course, there is a 
common agreement on the  purpose of such a sentence as a way of 
setting the mood of a cyberpunk novel, but the perception that each 
reader has certainly varies given the situatedness of language.&lt;/p&gt;

&lt;p&gt;Another example is H.P. Lovecraft, who constantly uses the concept of color (or the lack of it) to describe the universes where his solitary characters are transported to:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A rather large congeries of iridescent, prolately spheroidal bubbles and a very much smaller polyhedron of &lt;strong&gt;unknown colours&lt;/strong&gt; and rapidly shifting surface angles (The Dreams of the House of the Witch)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The vast tomb, or temple, was an &lt;strong&gt;anomalous color&lt;/strong&gt; — a nameless blue-violet shade (The Tree on the Hill)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Light filtered down from asky of &lt;strong&gt;no assignable colour&lt;/strong&gt; in baffling, contradictory directions, and played almost sentiently over what seemed to be a curved line of gigantic hieroglyphed pedestals more hexagonal than otherwise and surmounted by cloaked, ill-defined Shapes (Through the Gates of the Silver Key).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For Lovecraft,  the inability to identify or generate a meaningful description of color seems to be the unequivocal sign that something very bad, as usual, is going to happen. Like if being able to define a color meant for an individual to have control over the understanding of the reality.&lt;/p&gt;

&lt;p&gt;Besides the literary connotations, from a machine learning point of view, there is a whole line of research dedicated to this problem, which combines natural language processing, psychology and, even art theory.&lt;/p&gt;

&lt;p&gt;In general, the main idea is to use a set of (description, color) pairs to train a model able to map between these modalities. The used models differ in their nature but the task remains almost the same.  For example, we can mention the work of &lt;a href=&quot;https://aclweb.org/anthology/D16-1202&quot;&gt;Kawakami et al.&lt;/a&gt; which uses a character-level recurrent architecture to map a color description to an associated color. This tackles the particular problem that word-level models usually face, which is that color descriptions tend to be short, limiting the expressiveness of the models. &lt;a href=&quot;https://web.stanford.edu/class/cs224n/reports/2762012.pdf&quot;&gt;Bhargava et al.&lt;/a&gt; propose an autoencoder where the latent feature representation is aimed to align with a three-dimensional color vector. &lt;a href=&quot;https://aclweb.org/anthology/D16-1243&quot;&gt;Moore et al.&lt;/a&gt; focuses on the compositionality of the color descriptions by combining a neural encoder with a Fourier-based color transformer. The &lt;a href=&quot;https://transacl.org/ojs/index.php/tacl/article/view/1142&quot;&gt;same author&lt;/a&gt; incorporates a context perspective by modeling the description generation in a speaker-listener setting and also has recently &lt;a href=&quot;https://arxiv.org/pdf/1803.03917.pdf&quot;&gt;published&lt;/a&gt; an approach that incorporates bilingual data.&lt;/p&gt;

&lt;h3 id=&quot;from-language-to-colors&quot;&gt;From language to colors&lt;/h3&gt;

&lt;p&gt;Let’s start analyzing  Kawakami et al. paper, replicating the results and discussing possible directions. 
The first element to consider is the data. In this case, the authors propose to extract color-descriptions pairs from Colourlovers. Fortunately, there is an &lt;a href=&quot;http://www.colourlovers.com/api&quot;&gt;API&lt;/a&gt; and even a &lt;a href=&quot;https://github.com/elbaschid/python-colourlovers&quot;&gt;Python client&lt;/a&gt; and the extraction can be done with something like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;colourlovers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColourLovers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColourLovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cont&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# take it easy&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;cont&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;green&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After some hours, I was able to get around 800.000 pairs. I applied certain filters to remove duplicates and  characters that were very infrequent, leading to a set with the following characteristics:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/1.png&quot; alt=&quot;char&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/2.png&quot; alt=&quot;len&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The associated colors are transformed from RGB to Lab format, which allows us to arrange  in a 3-dimensional space. The spatial disposition of a sample is presented in the following animated graph:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;animate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# for a sample of 5000 colors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# initial angle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;facecolors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ani&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;animation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FuncAnimation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;animate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frames&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ani&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lab_space_90_30fps.gif'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'imagemagick'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/lab_space_90_30fps.gif&quot; alt=&quot;3dlab&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regarding the model, it consists of two main blocks. The first one, a color description encoder, takes a description of a color in natural language and pass it through a character-level LSTM, from which the last hidden state, &lt;script type=&quot;math/tex&quot;&gt;h \in \mathbb{R}^{300}&lt;/script&gt;  is used as a description learned feature vector. The second one, a feed-forward layer takes &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; and outputs the associated color in Lab format as &lt;script type=&quot;math/tex&quot;&gt;\hat{y} = \sigma(Wh+ b)&lt;/script&gt;, with &lt;script type=&quot;math/tex&quot;&gt;W \in \mathbb{R}^{3 \times 300}&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;b \in \mathbb{R}^3&lt;/script&gt;.
The model tries to minimize the mean squared error between the output color and the reference using backpropagation. While I tried several optimizers, Adam was the one with a more consistent performance on  the validation set, as seen in the following comparison:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/curves.png&quot; alt=&quot;curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial results show coherence between the generated colors and their references. If we analyze some of the color transitions as the network passes through the descriptions, we can see how intermediate colors are also aligned with the incomplete descriptions. This shows the expressive power of the character level approach, as the generation is a sequential process at a more fine grained granularity.&lt;/p&gt;

&lt;p&gt;Now, let’s take a look at  how  the hidden vectors  &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; produced  by the recurrent encoder change as we incrementally add more characters. For example,&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'c',
'co',
'com',
'comu',
'comua',
'comuan',
'comuanz',
'comuanza',
'comuanza ',
'comuanza g',
'comuanza gr',
'comuanza gre',
'comuanza gree',
'comuanza green'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'K',
'Ki',
'Kit',
'Kitc',
'Kitch',
'Kitche',
'Kitchen'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s start with a more qualitative approach, by obtaining the  intermediate feature vectors for each description and visualizing them in a two-dimensional space. Hopefully, we will be able to identify certain trajectories in vector space and their relationship to the associated colors.&lt;/p&gt;

&lt;p&gt;For example, for the color with the description &lt;strong&gt;&lt;em&gt;strawberry kiss&lt;/em&gt;&lt;/strong&gt;, we can obtain the vector from all its subsequences and reduce them altogether via PCA to generate the following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/strawberry_kiss.png&quot; alt=&quot;sk&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The associated transitions are presented in the following animation, where we can see that as we progress on the addition of characters, the distance between the vectors begin to decrease. This makes sense as at the beginning there is not restriction imposed by the conditional probabilities, but as we progress, the context associated to the previously seen characters narrows
the space of possible next characters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/sk5.gif&quot; alt=&quot;ska&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, the colors associated to each sub-sequence also converges. One thing to notice is, for example, in the word &lt;strong&gt;&lt;em&gt;strawberry&lt;/em&gt;&lt;/strong&gt;, when we get to the subsequence &lt;strong&gt;&lt;em&gt;straw&lt;/em&gt;&lt;/strong&gt;,  the color produced is between &lt;em&gt;light brown&lt;/em&gt;  and &lt;em&gt;pale yellow&lt;/em&gt;, which is characteristic to a &lt;em&gt;straw&lt;/em&gt; from  an agricultural perspective. As we continue adding characters, we can see how the color moves to a more red-ish spectrum, as &lt;strong&gt;&lt;em&gt;strawberry&lt;/em&gt;&lt;/strong&gt; is formed.  The same type of behavior can be seen in most of the descriptions.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;&lt;em&gt;Dijon mustard&lt;/em&gt;&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/dijon_mustard_ii.png&quot; alt=&quot;mustard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;&lt;em&gt;burnt blueberry&lt;/em&gt;&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/burnt_blueberry.png&quot; alt=&quot;burnt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A more interesting experiment is to visualize how two or more descriptions interact as their respective characters are appended. In that sense, let us assume a subset of descriptions that share a certain substring. As the generation is conditioned by the relative position of the characters, we can study the cases where the substring appears i) at the beginning of the descriptions (e.g. as &lt;strong&gt;&lt;em&gt;blue&lt;/em&gt;&lt;/strong&gt; in &lt;strong&gt;&lt;em&gt;blueberry&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;blue ocean&lt;/em&gt;&lt;/strong&gt;),  ii) in the middle or iii) at the end.&lt;/p&gt;

&lt;p&gt;For the  fist case, let us select the substring &lt;strong&gt;&lt;em&gt;gre&lt;/em&gt;&lt;/strong&gt; and sample ten descriptions from our test set the begin with such pattern:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'gre',
'greased lips',
'greasy spoon',
'great',
'great dark spot',
'great fright',
'great green',
'great lover',
'great party favors',
'great wide tea'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case, the resulting visualization is the following. 
(It must be noted that for the case of  two or more descriptions, the images generated by PCA are not expressive enough,
as most of the points are overlapped. For these cases, t-SNE performs much better.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/gre_set10.png&quot; alt=&quot;gre10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the corresponding animation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/gre5.gif&quot; alt=&quot;gre10ani&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the second case, if we select the substring &lt;strong&gt;&lt;em&gt;ran&lt;/em&gt;&lt;/strong&gt;, we obtain following sample set:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'Orange Creamsicle',
'arugula granita',
'orange scribbles',
'Light Amaranth',
'goranluppo',
'my random',
'orange celosia',
'Violet Fragrance',
'Pomegranate',
'red orange'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/ran.png&quot; alt=&quot;gre10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the last case, if we sample descriptions that end in &lt;strong&gt;&lt;em&gt;nge&lt;/em&gt;&lt;/strong&gt;, we obtain:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'Fringe',
'Lounge',
'Orange',
'avenge',
'change',
'fringe',
'lounge',
'orange',
'sponge',
'tounge'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/end_nge.png&quot; alt=&quot;end_nge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/end_nge_ani2.gif&quot; alt=&quot;end_nge_ani&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Visualizing the results, we can see the most interesting trajectories are generated when we define a substring  at the end, in the sense of obtaining trajectories that are more linear and encapsulated in certain zones of the vector space than in other cases.&lt;/p&gt;

&lt;p&gt;If you want to train your own model, the source for transferring from color names to LAB format can be found &lt;a href=&quot;https://github.com/pabloloyola/name-color&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, in this post we analyzed  the problem of producing a color given  a specific description of it. There are several extensions we can think of, but that is left to the reader’s imagination.&lt;/p&gt;

&lt;h3 id=&quot;from-colors-to-language&quot;&gt;From colors to language&lt;/h3&gt;

&lt;p&gt;Now we will try to do the inverse task, which means, give a color, expressed as a three-dimensional vector, obtain its description in natural language. To do that, we will take as inspiration  the approach described in &lt;a href=&quot;https://arxiv.org/abs/1606.03821&quot;&gt;Monroe et al.&lt;/a&gt; and try to obtain a working  version in Pytorch. The current implementation is available here: https://github.com/pabloloyola/color-description&lt;/p&gt;

&lt;p&gt;The data we will use in the first place is the dataset from &lt;a href=&quot;https://blog.xkcd.com/2010/05/03/color-survey-results/&quot;&gt;Munroe&lt;/a&gt;, which are the results of a survey where participants were asked to provide a short description of a given color. 
The actual version of the dataset is the one processed by &lt;a href=&quot;https://aclweb.org/anthology/Q15-1008&quot;&gt;McMahan et al.&lt;/a&gt; and that is available for download &lt;a href=&quot;http://paul.rutgers.edu/~bcm84/rugstk_v1.0.tar.gz&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The original representation of the colors is the HSV format, which stands for Hue, Saturation and Value. This format is intended to more closely resemble how humans perceive color attributes. Here is a small sample from the training set:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/munroe-data.png&quot; alt=&quot;munroe-data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the nature of the data collection, there are cases where one color description is associated to several –very similar– color vectors, and also cases  where one color vector is related to more than one color description.&lt;/p&gt;

&lt;p&gt;As described in the original paper, the model consists of an LSTM decoder that receives as input a color in the HSV format and the associated description, as a sequence of words.&lt;/p&gt;

&lt;p&gt;At each time step the LSTM is fed with a vector resulting from concatenating the vector representation of the color  we want to model, and the embedding vector associated to the word predicted by the model in the previous step. The output state from the LSTM is fed into a softmax layer from which the next word is selected.&lt;/p&gt;

&lt;p&gt;One interesting element to consider is  how to treat the color representation. We can just pass the raw three-dimensional vector as it is, assuming the the HSV space is sufficient to provide expressive representations. Another alternative is presented in the original paper, where it is proposed to transform the HSV vectors into a Fourier basis representation, resulting in  a 54-dimensional vector.&lt;/p&gt;

&lt;p&gt;An initial parameter search led to feasible results. Probably because the vocabulary size is not so big (as well as the sequences not so long). This is a random list showing a subset of the test set. The first column is the actual generated color description while the second and  third column represents that reference, in terms of the description and the color in RGB format.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/atomic.png&quot; alt=&quot;atomic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the model tends to output descriptions that while do not match exactly with the reference, they clearly resemble the target color. It is interesting to see the small perceptual differences between &lt;code class=&quot;highlighter-rouge&quot;&gt;orange&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;salmon&lt;/code&gt; and  &lt;code class=&quot;highlighter-rouge&quot;&gt;pink&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;magenta&lt;/code&gt;. 
One thing to notice in this case, is that the color descriptions are mostly formed by one word (maybe we should just call them color names in that case). This is a natural consequence of the nature of the dataset, where people were asked to be specific in their descriptions.&lt;/p&gt;

&lt;p&gt;If we filter a bit and collect results  where the generated sequences have length more than one (ignoring  &lt;code class=&quot;highlighter-rouge&quot;&gt;pad&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bos&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;eos&lt;/code&gt; special tokens), we can see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/not-atomic-blue.png&quot; alt=&quot;not-atomic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This a sample mostly from the &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt; spectrum, where can see two interesing things. Firstly, the accompanying words, usually adjectives, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;dark&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;light&lt;/code&gt;, are a bit repetitive, but effective. For example, in the third instance, we can see that the reference is just  &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt;, but if we look at the actual color, the generated description, &lt;code class=&quot;highlighter-rouge&quot;&gt;light blue&lt;/code&gt; seems to fit better. Of course,  we do not have a quantitative way to measure this as we do not have a point of reference of what is actually &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt;, but the model seems to learn globally such characteristics.&lt;/p&gt;

&lt;p&gt;If we filter a  bit more and consider only the generated descriptions and the references have more than one token, we obtain the following list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/2-2-green-blue.png&quot; alt=&quot;2-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, we see a more uniform matching between generated and reference descriptions.&lt;/p&gt;

&lt;p&gt;One interesting aspect to study is the impact of the Fourier transformation of the color vectors. The following graph compares the training and testing losses, showing that in fact expanding to a 54 dimensional representation improves model performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/losses_color_desc.png&quot; alt=&quot;losses_color_desc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Having explored the word-level approach, the natural next step is to
study if we can go a more fine grained level, for example, study if we can generate description character by character.
While this could represent an extra challenge for the model, I think it could also open the door to obtain more 
diverse descriptions. Let’s see.&lt;/p&gt;

&lt;p&gt;The most straightforward way to obtain a character level approach is to reuse the exsiting code and just change the tokenizer function to split the description into a sequence of characters.&lt;/p&gt;

&lt;p&gt;When I  was playing with the hyperparameters, something that caught my attention was how to structure the input vector for the decoder. As we saw above, this vector is the concatenation of the color representation (raw three-dimensional vector or an expansion based on the Fourier mapping) and the embbeded vector of the word predicted in the previous step.  For the word-based level, changing the proportion of the two components did not impact on the results. Something different I experienced in the character-level method. For example, the following figure compares the training and testing losses between three runs, using vectors of 50, 100 and 200 respectively. Again, the first column represents the generated description, and the second and third , the references.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/losses_embedding_size_character_level.png&quot; alt=&quot;losses_embedding_size_character_level&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Such difference also impact on the generated descriptions, for example:&lt;/p&gt;

&lt;p&gt;Sample of generated descriptions with embedded vector of size 50:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/size50.png&quot; alt=&quot;size50&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample of generated descriptions with embedded vector of size 100:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/size100.png&quot; alt=&quot;size100&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample of generated descriptions with embedded vector of size 200:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/learn-color-reps/size200.png&quot; alt=&quot;size200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the size of the character embedding indeed impacts on the generation results. A vector of size 50 seems to generate incomplete descriptions, meaning the it is not expressive enough to guide the generation to known descriptions.  A size of 200 totally makes the model collapse, generating the same pattern for the majority of the instances on the test set. Finally, a size of 100 seems to  provide much reliable results, in this case we can see the both the signal from the color and the embedded vector play nicely together, such as in the cases of &lt;code class=&quot;highlighter-rouge&quot;&gt;red&lt;/code&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;orange-brown&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;brown&lt;/code&gt;  for  &lt;code class=&quot;highlighter-rouge&quot;&gt;khaki&lt;/code&gt;, which means, while the model cannot always produce  the exact description, at least it is able to use in a effective way the color representation to find a feasible alternative.&lt;/p&gt;

&lt;p&gt;All the code and data generation scripts can be found here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://github.com/solargrammars/name_color&quot;&gt;https://github.com/solargrammars/name_color&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://github.com/solargrammars/color_descriptions&quot;&gt;https://github.com/solargrammars/color_descriptions&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Solar Grammars</name></author><category term="blog" /><summary type="html">Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based on that, contribute to understanding the principles of language acquisition. In that sense, color description, or more generally, the generation of color language, is a relevant aspect for understanding the emergence of human expression.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/learn-color-reps/gre_set10.png" /></entry></feed>