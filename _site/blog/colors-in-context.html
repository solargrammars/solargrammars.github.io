<!doctype html>
<html>

<head>

  <title>
    
      Palettes and Graphs as Color Context | Solar Grammars
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <!--<link type="application/atom+xml" rel="alternate" href="http://pablomentat.duckdns.org:4000/rss-feed.xml" title="Solar Grammars" />-->
  
  <link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Solar Grammars | "/>
  


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-163457888-1', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Palettes and Graphs as Color Context | Solar Grammars</title>
<meta name="generator" content="Jekyll v3.6.3" />
<meta property="og:title" content="Palettes and Graphs as Color Context" />
<meta name="author" content="Solar Grammars" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Colors are usually perceived as part of a composition. It is rare that we can find them in isolation in nature. Sometimes we find articles, usually associated to marketing, where people make a direct relationship between a color and an emotion or a feeling. For example, when the color red is associated to passion and green to hope. Such relationships could be true to some extent, but I think it is also important to consider the context associated to such perceptions. A red color indeed can express passion, but in some cases, given the set of other colors present in the composition, it could also express sadness or fear." />
<meta property="og:description" content="Colors are usually perceived as part of a composition. It is rare that we can find them in isolation in nature. Sometimes we find articles, usually associated to marketing, where people make a direct relationship between a color and an emotion or a feeling. For example, when the color red is associated to passion and green to hope. Such relationships could be true to some extent, but I think it is also important to consider the context associated to such perceptions. A red color indeed can express passion, but in some cases, given the set of other colors present in the composition, it could also express sadness or fear." />
<link rel="canonical" href="http://pablomentat.duckdns.org:4000/blog/colors-in-context.html" />
<meta property="og:url" content="http://pablomentat.duckdns.org:4000/blog/colors-in-context.html" />
<meta property="og:site_name" content="Solar Grammars" />
<meta property="og:image" content="http://pablomentat.duckdns.org:4000/arctic-1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-01-03T00:00:00+09:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Palettes and Graphs as Color Context","dateModified":"2017-01-03T00:00:00+09:00","datePublished":"2017-01-03T00:00:00+09:00","author":{"@type":"Person","name":"Solar Grammars"},"url":"http://pablomentat.duckdns.org:4000/blog/colors-in-context.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://pablomentat.duckdns.org:4000/blog/colors-in-context.html"},"image":"http://pablomentat.duckdns.org:4000/arctic-1.jpg","description":"Colors are usually perceived as part of a composition. It is rare that we can find them in isolation in nature. Sometimes we find articles, usually associated to marketing, where people make a direct relationship between a color and an emotion or a feeling. For example, when the color red is associated to passion and green to hope. Such relationships could be true to some extent, but I think it is also important to consider the context associated to such perceptions. A red color indeed can express passion, but in some cases, given the set of other colors present in the composition, it could also express sadness or fear.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

<div class="container">
  <header class="site-header">
  <h3 class="site-title">
    <!--
    <img  src="../assets/img/SG.jpg" style="width:50px;max-width:50px;float:left;margin:5px 10px 0 0;">
    -->
    <a href="/">Solar Grammars</a>
  </h3>
  <nav class="menu-list">
    
      <a href="/pages/contact.html" class="menu-link">Contact</a>
    

    
      <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    
      <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    
      <a href="rss-feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
    
  </nav>
  <div class="dropdown">
    <button class="dropbtn"><i class="fa fa-bars" aria-hidden="true"></i></button>
    <div class="dropdown-content">
      
        <a href="/pages/contact.html" class="menu-link">Contact</a>
      

      
        <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      
        <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
      
        <a href="rss-feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
      
    </div>
  </div>
</header>

  <div class="posts-wrapper">
    <div class="page-content">
  <h1>
    Palettes and Graphs as Color Context
  </h1>

  <span class="post-date">
    Written on
    
      Solar Grammars
    
  </span>

  <!--
  
    <div class="featured-image">
      <img src="/assets/img/arctic-1.jpg">
    </div>
  
  -->
  <article>
    <p>Colors are usually perceived as part of a composition. It is rare that we can 
find them in isolation in nature. Sometimes we find
articles, usually associated to marketing, where people make a direct relationship
between a color and an emotion or a feeling. For example, when the color
red is associated to <code class="highlighter-rouge">passion</code> and green to <code class="highlighter-rouge">hope</code>. Such relationships could
be true to some extent, but I think it is also important to consider the context associated
to such perceptions. A red color indeed can express passion, but in some cases, given 
the set of other colors present in the composition, it could also express sadness or fear.</p>

<p>In that sense,  probably what colors evoke is 
tightly associated to the perception of a colorful <code class="highlighter-rouge">context</code>. In other words, how we perceive 
a certain type of red, does not depend only on such color, but also on the colors
that surround it.</p>

<p>As we are interested in learning feature representations of colors, we could
try to come up with a setting where colors appear as a group and then find associations
and, hopefully, patterns of co-occurrence between them.</p>

<p>How can we find groups of colors we can work on? On simple way is to
collect a large amount of RGB images and  from each of them extract a color palette. A color
palette basically compresses an image into the most representative colors. We could say 
it is the most primitive way to do compression. While a palette cannot
provide a representation of the semantics of the image, at least it can inform us about
the main colors used and their distribution.</p>

<p>Let’s assume we have a set of images from which we can extract a color palette of <script type="math/tex">n</script> colors, <script type="math/tex">p_i = \{ c_1, ..., c_n \}</script>. 
With such dataset, we could conduct a simple experiment: We know that in natural language, 
we can assume that words that occur in the same contexts tend to have the similar meaning, i.e.,
<em>you can know a word by the company it keeps</em>, which is known as the distributional hypothesis. 
Can we transfer such idea to colors, using the palettes as context?  In other words, 
can  we make the same analogy, and  say that colors that tend to co-occur  share certain
semantics? For example, let’s say we collect thousands of pictures of bathrooms. In most
of them <code class="highlighter-rouge">white</code>, <code class="highlighter-rouge">light blue</code> and <code class="highlighter-rouge">pink</code> will appear quite frequently, because there seems to be 
a common agreement on  how bathrooms look like, like a pattern in which such colors represent the
<code class="highlighter-rouge">cleanness</code> and <code class="highlighter-rouge">calmness</code> a bathroom should have. Same if we analyze pictures of forests, probably
<code class="highlighter-rouge">greens</code> and <code class="highlighter-rouge">browns</code> will be predominant, while <code class="highlighter-rouge">pinks</code> will be very unlikely to appear. Can 
we leverage such regularities to learn dense feature vectors for each color?</p>

<p>Let’s start with a very simple approach, using very well known tools to see if we are heading
in a feasible direction. Firstly, we need some data.  For simplicity, let’s use the validation set 
of the Microsoft COCO dataset. For each image, let’s capture a color palette of 6 colors.</p>

<p><img src="/assets/img/blog/colors-in-context-img/p1.jpg" width="400px" /></p>

<p>For the above image, let’s compute a incremental palettes. As we can see, the colors are appended
in terms of their absolute proportion on the image.</p>

<p><img src="/assets/img/blog/colors-in-context-img/p21.png" style="height:50px;" /> <br />
<img src="/assets/img/blog/colors-in-context-img/p22.png" style="height:50px;" /> <br />
<img src="/assets/img/blog/colors-in-context-img/p23.png" style="height:50px;" /> <br />
<img src="/assets/img/blog/colors-in-context-img/p24.png" style="height:50px;" /> <br />
<img src="/assets/img/blog/colors-in-context-img/p25.png" style="height:50px;" /> <br />
<img src="/assets/img/blog/colors-in-context-img/p26.png" style="height:50px;" /></p>

<p>With this data, we can test a direct analogy from natural language BOW models.
Let’s recall such formulation as , let’s say we have a sentence <script type="math/tex">s</script> composed
by <script type="math/tex">n</script> words , <script type="math/tex">s  = \{  w_1, w_2 , ... , w_n \}</script>, then a CBOW approach
learn vectors <script type="math/tex">v_i</script> for a word <script type="math/tex">w_i</script> as maximizing the likelihood 
of <script type="math/tex">w_i</script> given its neighborhood  <script type="math/tex">\{ w_{i-c} , ... , w_{i+c} \}</script>.</p>

<p>One problem we face at this moment is that each exact color probably appear only once
in one palette in all the dataset.  In other words, while we can
have two red colors that  look quite similar, in reality their RGB
representation will be different and therefore a model will perceive
them as different. To solve this issue, we can discretize their representation by clustering them.</p>

<p>Then, for example, we can expect  all variations of <code class="highlighter-rouge">scarlet</code> color to be grouped on a single 
cluster. Therefore, on a palette that contains one of these scarlet variations, instead of 
using the actual color, we can use the centroid/medoid of the associated cluster.</p>

<p>Let’s see first if clustering colors makes sense. For simplicity we can take the RGB or LAB 
tuples as feature vectors an apply K-means. In order to get a
rough estimation of the optimal number of clusters, we can take a look at both 
 distortion ( the average of the squared distances from the cluster centers of the respective 
 clusters ) and   inertia  (the sum of the squared distances to the closest cluster center). For 
 a sample of the data, we get the following: 
<img src="/assets/img/blog/colors-in-context-img/num_clusters_study2.png" alt="num_clusters_study" /></p>

<p>Which roughly tells us a good number of clusters is around 25. This makes sense as we are working 
with very standard natural images from COCO, so the colors they present should be quite compact. 
This result was obtained using a sample from 4000 images from which palettes of six colors were 
extracted, conforming a group of around 22.000 total colors that we clustered using their RGB 
representation. Changing  the amount of images or using the LAB format as feature vector, did 
not change considerably  the results.</p>

<p>Let’s take a look at the clusters visually. With <script type="math/tex">k</script> = 25, we can see something like this:</p>

<p><img src="/assets/img/blog/colors-in-context-img/cluster_palettes/0.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/1.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/2.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/3.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/4.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/5.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/6.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/7.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/8.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/9.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/10.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/11.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/12.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/13.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/14.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/15.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/16.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/17.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/18.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/19.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/20.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/21.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/22.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/23.png" style="float:left;" /><br />
<img src="/assets/img/blog/colors-in-context-img/cluster_palettes/24.png" /></p>

<p>Here we have 20 random elements from each cluster. As we can see, 
the groups tend to make sense, as there is a clear distinction between them and also a inherent cohesion within each cluster. Of course we 
can refine and adapt the number of clusters  based on the requirements of any  downstream task, but let’s use this result for the moment.</p>

<p>With this, we can  associate each color with its cluster id so 
we can generate  the reduced palettes. Now, let’s learn a vector representation from each element in the reduced
palettes, i.e., a dense vector for each cluster id. We want to exploit the co-occurrence of the colors in the palettes
making a direct analogy as how words co occur in a set of sentences. In that sense we can train the reduced palettes in
the same way sentences are used in CBOW:</p>

<p><img src="/assets/img/blog/colors-in-context-img/diagram2.png" alt="diagram1" /></p>

<p>As stated above, the idea is to learn a feature vector for each color <script type="math/tex">c_i</script> as maximizing the likelihood 
of <script type="math/tex">c_i</script> given its neighborhood  <script type="math/tex">\{ c_{i-\lambda} , ... , c_{i+\lambda} \}</script>:</p>

<script type="math/tex; mode=display">P(c_i | \{ c_{i-\lambda} , ... , c_{i+\lambda} \} )</script>

<p>CBOW serves as a nice inspiration, as there is no dependency between the elements in the context window. 
In our setting,  we also don’t want to enforce any ordering, as in the palette, we cannot assume the colors 
conform a sequence. Something interesting to notice is that when we obtain the original palettes, for each 
color we can obtain its proportion ( between 0 and 1) . This value
could be incorporated as a weight when we sum context vectors.</p>

<p>One of the parameters that impacts the most on the performance of CBOW is actually the size of the context 
window. In NL, larger windows tend to favor more topical similarities, while shorter ones  privileges more 
functional similarities. In the context of this problem, we don’t know in advance such effect, therefore it 
will be necessary to explore empirically if there is any difference.</p>

<p>For palettes of six colors, and considering 25 clusters,  moving the window context size from 2 to 5 really 
does not any noticeable impact. But when we start using  longer palettes, the differences are visible.
For example, here we show a small experiment with palettes of 20 colors and a number of clusters equal to 400. In 
the following figures, each learned representation is reduced to a two dimensional point space using TSNE . I
have associated to each point the original RGB color, so we can visualize the vectors in terms of their colorful nature.</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_2.png" /></td>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_3.png" /></td>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_4.png" /></td>
    </tr>
    <tr>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_5.png" /></td>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_6.png" /></td>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_7.png" /></td>
    </tr>
    <tr>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_8.png" /></td>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_9.png" /></td>
      <td><img src="/assets/img/blog/colors-in-context-img/window_sizes/context_size_10.png" /></td>
    </tr>
  </tbody>
</table>

<p>Based on the above, we can question ourselves a bit and 
try to ask what would make a good context-informed color
representation? Is it ok if we obtain vectors that associate 
similar colors  into well defined groups? In some sense yes, 
but for such thing probably just using the RGB or LAB 
representation of the colors is enough. I think while it is
important  that the learned representations allow us to relate
similar colors, at the same time  I find it equally important that
these representation also encode the closeness of the colors 
in the context of the images the come from. Going back 
to the example we talked before, if we analyze images
of landscapes of forests, we probably can see groups of greens (let’s
say , representing trees or vegetation ) closer to groups
of light blues and whites on one side (the clouds and sky)
and also some groups of brown and grays (the ground). 
It is not entirely clear which configuration is better, but
from the results, I would probably choose something between 4 and 5
as a context window.</p>

<p>At this point, we have a vector representation the can 
be associated to the whole cluster. Thats fine, but what we really
want is a vector for each color.  What we can do is to assume
that the learned vector is actually associated to the cluster
centroid (or medoid), then extrapolate the vector associated to 
each color in the cluster in terms of its distance to the centroid.</p>

<p><img src="/assets/img/blog/colors-in-context-img/diagram1.png" alt="diagram1" /></p>

<p>In the above figure, let’s say for cluster <script type="math/tex">k</script>  we obtained a vector representation
<script type="math/tex">v_k</script>. Let <script type="math/tex">c_i</script> be the medoid color and we arbitrarily associate <script type="math/tex">v_k</script> to that point.
Then, we need to find a way to obtain a vector to all the other $c_j$ members of the cluster, taking
into account <script type="math/tex">v_k</script> as reference. A natural way to proceed would be to use the distance  <script type="math/tex">d_{ij}</script> between <script type="math/tex">c_i</script> and
<script type="math/tex">c_j</script> as a way to weight <script type="math/tex">v_k</script>, assuming that if the entities keep a certain relationship 
in the the RGB space, such relationship should be also present, to some extent,  in a learned 
feature space. Then, to produce a feature vector associated to <script type="math/tex">c_j</script>, such representation
will have to keep certain direct proportion such as:</p>

<script type="math/tex; mode=display">dist(c_i, c_j) \propto dist(v_k, v_j)</script>

<p>Such  relationship probably is not linear as we are trying to relate  two dissimilar  feature spaces : the color one  (roughly assuming that RGB or LAB  spaces can be metric space … which strictly speaking is not correct), and the one defined by the feature representations from the co occurrence in the palettes. Finding  a factor that generalizes well across examples probably is hard. One way to proceed is parametrize such relationship via a small neural network. In that sense, we take all the pairs (centroid, vector)
and train a network. Then, at inference time, we pass all the real colors from to obtain a dense representation.</p>

<p>Once we have obtained a vector representation for all colors present in our dataset, we can 
start visualizing them. The first thing we can do is to reduce the 300-dim vectors into two dimensions and plot them.  In the following figure, we can see each point to which we have associated its original color in the visualization.</p>

<p>The first thing we can notice is the smooth transition across colors in most of the graph. Remember each point location is not
their RGB , but the learned dense vector. In that sense we can see that the greens seems to be quite close the 
certain variation of light brown/ orange, probably representing the co occurrence of landscapes. The same phenomena can be seen with the light blues and whites, whose closeness probably come from the disposition of those colors
in the sky. Interestingly, the fact that the blacks and browns appear in the center, means the that they tend to co occur with all other groups, which makes sense as in natural images darker colors are associated
with transversal elements such as shadows that appear across all images.</p>

<p><img src="/assets/img/blog/colors-in-context-img/color_vectors_2d.png" alt="color_vectors_2d" /></p>

<p><img src="/assets/img/blog/colors-in-context-img/3d.png" alt="color_vectors_2d" /></p>

<h2 id="possible-improvements-optimizations">Possible Improvements/ Optimizations</h2>

<ul>
  <li>
    <p><em>Single model:</em> Our model is currently split into two main blocks. The fact the we have to perform a clustering
to reduce the palettes, while can be seen as a limitation, it also allows us to have more control, like a real pipeline. What
we can do  is try to merge the steps into one single model, by means of working directly with the original colors
from the palettes, without having to apply any reduction.</p>
  </li>
  <li><em>Vector arithmetics:</em> if we combine yellow and green in the RGB space,  we probably will obtain <a href="https://youtu.be/0fC1qSxpmKo">some kind of blue</a>… most likely.  What if we combine the learned vectors associated to yellow and green? Will the resulting vector be similar to the learned vector of blue too?</li>
  <li><em>Segmenting COCO or using other sources of images:</em> COCO provides a set of very heterogeneous images: people, landscapes, sports etc. What if we run this pipeline on a subset of images that share the same semantics? Let’s say, only on images of landscapes, or only images of restaurants. In those cases, for a given color, how the learned
representations obtained from different sets relate?</li>
</ul>

<h2 id="incorporating-natural-language">Incorporating Natural Language</h2>

<p>So far, we have been considering only the images and their colors. But the COCO dataset
provides a set of captions for each image, a source of data that looks quite irresistible.
Then the question is, how can we incorporate such information? How the representations of the colors
could be <code class="highlighter-rouge">improved</code> by informing our model/pipeline about the caption associated to the images we
use to obtain the palettes. I don’t know the answer yet, but let’s explore a bit to see in which 
direction we should move.</p>

<p>Let”s not make things difficult and keep working with the validation part of COCO. For each image, we 
can obtain one or more captions.  The from each image <script type="math/tex">i \in I</script>, we can obtain a 
pair <script type="math/tex">(p_i, s_i)</script>, where <script type="math/tex">p_i = \{ c_1, ..., c_N \}</script> is  the associated palette of <script type="math/tex">N</script> colors, 
and <script type="math/tex">s_i = \{  w_1, ..., w_M \}</script> the caption expressed as a sequence of  words. For each 
of these captions we can use GLOVE (or any other source of pretrained vectors) to
obtain  a good dense representation for each word.</p>

<p><img src="/assets/img/blog/colors-in-context-img/diagram3.png" alt="diagram3" /></p>

<p>Given this extra source of information, how could we use the co occurrence patterns associated to 
the words to support the context based representation learning of colors? We know there is a one 
to one relationship between the sentences and the palettes:</p>

<p><img src="/assets/img/blog/colors-in-context-img/diagram4.png" alt="diagram4" /></p>

<p>Hmmm it seems there is going to be necessary to obtain a representation of the sentence, <script type="math/tex">s_i</script>. That 
is not difficult, as we can just do a plain average of the word vectors in <script type="math/tex">s_i</script>, use something 
like <a href="http://proceedings.mlr.press/v32/le14.pdf">Doc2vec</a> or if we care about the dependencies (we probably should), 
we can use an RNN to encode the sentence and  capture the last hidden state as a representation 
of the sentence.  In any case, let”s assume that for each sentence <script type="math/tex">s_i</script> we obtain a dense sentence vector <script type="math/tex">sv_i</script>.</p>

<p>Let”s start with a very simple experiment. For each pair <script type="math/tex">(p_i, sv_i)</script> let”s use kNN to obtain the <script type="math/tex">k</script> 
closest pairs, based on the cosine similarity between sentence vectors</p>

<script type="math/tex; mode=display">p_i, sv_i    \rightarrow kNN_{sv_i} = \{  sv_1, sv_2 , ... , sv_K\}</script>

<p>Each of these sentence vectors have in turn associated a palette. Therefore, we can , 
transitively, associate the palette <script type="math/tex">p_i</script> with a set of <script type="math/tex">K</script> palettes based on the 
underlying similarity of their associated sentences.</p>

<p><img src="/assets/img/blog/colors-in-context-img/diagram6.png" alt="diagram6" /></p>

<p>For example, for the following  instance:</p>

<p><code class="highlighter-rouge">a man in mid air attempting to catch a frisbee</code><br />
<img src="/assets/img/blog/colors-in-context-img/palette-caption-q.png" alt="pcq" /></p>

<p>the closest five pairs are:</p>

<p><code class="highlighter-rouge">a man reaching out to catch a frisbee</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc1.png" alt="pcq" /><br />
<code class="highlighter-rouge">a man jumps to catch a frisbee flying through the air</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc2.png" alt="pcq" /><br />
<code class="highlighter-rouge">there is a dog in the air going to catch a frisbee</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc3.png" alt="pcq" /><br />
<code class="highlighter-rouge">a man in a grassy field about to catch a frisbee</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc4.png" alt="pcq" /><br />
<code class="highlighter-rouge">a man is in mid air doing a skateboard trick</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc5.png" alt="pcq" /></p>

<p>We can see that while simple, the sentence embedding is able 
to capture quite relevant semantic relationships between sentences. Of course,
it degrades quite quickly, but that a is characteristic of the data under 
study.</p>

<p>More interesting is the relationship between the colors. At first glance
I cannot see any direct similarity at palette level on this example. Actually, it looks
pretty random. I think this is because for this sample, the 
core word is <code class="highlighter-rouge">frisbee</code>, which inherently does not have an associated color, right?
What if we sample the captions by a word that can be easily associated to
a color, for example <strong>forest</strong>, which I expected to be linked to greens.</p>

<p>For a given sample containing the <strong>forest</strong> word:</p>

<p><code class="highlighter-rouge">a bench in the middle of a lush green forest</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc6.png" alt="pcq" /></p>

<p>We obtain the following five closest:</p>

<p><code class="highlighter-rouge">a tree sitting in the middle of a lush green field</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc7.png" alt="pcq" /></p>

<p><code class="highlighter-rouge">a road in the middle of a beautiful green forest</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc8.png" alt="pcq" /></p>

<p><code class="highlighter-rouge">a large brown cow standing on the side of a lush green hill</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc9.png" alt="pcq" /></p>

<p><code class="highlighter-rouge">a man and child standing on top of a lush green field</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc10.png" alt="pcq" /></p>

<p><code class="highlighter-rouge">a photo of a bench in the middle of the beach</code><br />
<img src="/assets/img/blog/colors-in-context-img/pc11.png" alt="pcq" /></p>

<p>While there is still quick degradation on the semantic 
similarity across sentences, at least we can see a bit
more cohesion at color level.</p>

<p>This is getting more interesting, but I do not a clear 
approach to treat the high variability ( in  other words, 
the randomness) of the color-token relationship in this
configuration.</p>

<h2 id="more-structure-through-graphs">More Structure through graphs</h2>

<p>We have explored the feature learning  of colors by means of
capturing their co-occurrence using palettes. While palettes are 
easy to obtain, they do not encode any kind of structural information. 
In contrast, if we rely on graph we could have a richer setting
that could allow a more comprehensive learning.</p>

<p>How could we extract a color graph from an image? Actually, there 
are families of techniques from image processing that compute an
intermediate graph between image segments, for example, when 
performing image compression.</p>

<p>Let”s see some examples of color graphs obtained from both COCO dataset
samples as well from art pieces from the WikiArt website:</p>

<p><code class="highlighter-rouge">COCO</code>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco1.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco1_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco2.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco2_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco3.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco3_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco4.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco4_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco5.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/coco5_graph.png" /></td>
    </tr>
  </tbody>
</table>

<p><code class="highlighter-rouge">WIKIART</code>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki1.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki1_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki2.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki2_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki3.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki3_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki4.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki4_graph.png" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki5.png" /></td>
      <td style="text-align: center"><img src="/assets/img/blog/colors-in-context-img/image_graphs/wiki5_graph.png" /></td>
    </tr>
  </tbody>
</table>

<p>We can see that the resulting graphs represent a good discretization 
of the images, as they seems to cover the color spectrum  quite well.
The nodes position are not aligned with the actual image 
composition, but that can be added.</p>

<p>Having computed these graphs for a set of images, how can we learn
feature representations that encode the relationships between colors?
In other words, not only learn a dense vector for each node, but also 
for the edges that connect them.</p>

<p>One easy way would be to reuse most of the code we wrote before and 
re-apply a CBOW-like approach by sampling random walks from the graph
This is the idea behind existing approaches such  as <a href="https://arxiv.org/abs/1607.00653">Node2vec</a>.</p>

<p><img src="/assets/img/blog/colors-in-context-img/node2vec.png" alt="" /></p>

<p>One nice thing about this idea is that the random walks could allow us
to visualize the progression of colors in certain zones. But at the same time,
as we need to define in advance a window, that could be restricting 
long term color dependencies present on the images.</p>

<p>Let’s explore another alternative that is provided  by the family of 
graph neural networks. This family of models are based on a 
message passing schema, where we iteratively learn node representations
by propagating information through the edge structure. In that sense,
the representation for a given node at a given time is the result 
of the aggregation function on all its neighbor nodes.</p>

<p>Recently, there has been an extensive list of publications, 
each of them proposing a specific variation in terms of the how the
representations are computed. A paper by <a href="https://arxiv.org/abs/1704.01212">Gilmer et. al.</a> 
does a great job (basically showing 
that the myriads of papers that have appeared in the last years are … basically
the same) by
summarizing all the literature and providing a description of the core set of 
functionalities underneath the learning process. In short, if we assume
an undirected graph  <script type="math/tex">G</script>, node features <script type="math/tex">x_v</script> and edge features <script type="math/tex">e_{vw}</script>, 
we will have a message passing phase that runs for <script type="math/tex">T</script> time steps, 
defined in terms of a message function <script type="math/tex">M_t</script> and a vertex update function <script type="math/tex">U_t</script>. 
Hidden states <script type="math/tex">h_v^t</script> at each node are updated based on messages <script type="math/tex">m_v^{t+1}</script>,
which in turn is defined as <script type="math/tex">m_v^{t+1} = \sum_{w \in N(v)}M_t(h_v^t,h_w^t, e_{vw})</script>
and with  <script type="math/tex">h_v^{t+1} = U_t(h_v^t,m_v^{t+1})</script>.</p>

<p>A subsequent readout phase can compute feature vector for the whole graph, using readout, 
function <script type="math/tex">R</script>, <script type="math/tex">\hat{y} = R(\{ h_v^T|v \in G  \})</script>. 
Here, as you can image, <script type="math/tex">M_t, U_t, R</script> are learnable functions.</p>

<p>In our case, we can easily use this framework in a semi supervised learning setting.
For each graph associated to each image, we can let the model know the labels (RGB values)
only from a portion of the nodes. Then, the goal is to infer the RGB vector
associated to  each node, including both labeled and unlabeled.  The assumption
behind this is that the message passing mechanism will eventually be able to 
propagate information from the seen to the unseen nodes, taking into account 
the locality of the color composition expressed by the edge structure. This is an important
point to notice: colors in a composition such a natural image tend to form
neighborhoods, and they  can be as granular as we want, depending on the parameters
chosen when we compress the image.</p>

<p><img src="/assets/img/blog/colors-in-context-img/message_passing.png" width="200px" /></p>

<p>In the above figure, we expect the unlabeled node, represented in white, iteratively
receives a color signal from its neighborhood in the form of a node vector, 
which can be used to learn the correct RGB (or Lab) tuple.</p>

<p>Something noticed during the exploratory training is how the proportion of 
labeled nodes impacts on the performance of the color generation. Naturally, the more
nodes we have labeled, the better the results, as more information will be encoded in 
the graph. Here we can see how a small portion of nodes compare against a big one. 
While in terms of loss in the labeled set there no much difference, it is clear 
that  in the case of the unlabeled nodes, there is a advantage.</p>

<p><img src="/assets/img/blog/colors-in-context-img/losses3.png" />
<img src="/assets/img/blog/colors-in-context-img/losses4.png" /></p>

<p>Just the number of nodes is not enough, but its connectivity
is also very important. As we are in a message passing configuration, if the information
cannot be propagated from the labeled nodes to the unlabeled ones, the learning will 
be poor even if we consider a larger proportion of nodes with labels. In that sense,
average degree of the nodes we have adding is more relevant than the pure number of them. 
That is an interesting  design decision in terms of data acquisition when designing 
a machine learning model under this scenario</p>

<p>While the loss can help us to automate the the search for better parameters, does not
tell much about the actual results. Let’s take a look at some samples from the generated
colors. In the first place, we have the results for the labeled set. It seems to
be a easy task for the model, naturally. This is just a safe check, just to 
make sure we are not making any big mistake. In any case, I can see clear differences in terms of
brightness, specially in the case of pinks.</p>

<table class="table_colors" style="width:500px;">
<tr><td> Generated </td> <td>  Ground truth </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#7c9e7a"> </div> </td> <td>  <div style="width:200px; height:20px;background:#5f9177"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#732d83"> </div> </td> <td>  <div style="width:200px; height:20px;background:#8e4187"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#c7767c"> </div> </td> <td>  <div style="width:200px; height:20px;background:#9e7879"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#c18076"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#3e193c"> </div> </td> <td>  <div style="width:200px; height:20px;background:#3e0847"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#ce7c8a"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#7d80dd"> </div> </td> <td>  <div style="width:200px; height:20px;background:#9784d0"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#5a8f8b"> </div> </td> <td>  <div style="width:200px; height:20px;background:#46968b"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#e0609d"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#c5757f"> </div> </td> <td>  <div style="width:200px; height:20px;background:#9e7879"> </div> </td> </tr>
</table>

<p>Now, the more interesting result. Here we can see a sample of of the color generation for nodes that
from the unlabeled set, i.e., the produced color is the result of the label propagation through the link structure
of the graphs under study. The following  sample is obtained randomly, and it is presented sorted in terms
of the euclidean distance between  the generated and ground truth color, so we can visualize more effectively how the 
results degrade.</p>

<table class="table_colors" style="width:500px;">
<tr><td> Generated </td> <td>  Ground truth </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#499b93"> </div> </td> <td>  <div style="width:200px; height:20px;background:#46968b"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#e86d9e"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#5a8f81"> </div> </td> <td>  <div style="width:200px; height:20px;background:#46968b"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#db7c9b"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#72278c"> </div> </td> <td>  <div style="width:200px; height:20px;background:#8e4187"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#846a57"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#c78776"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#b2bd5e"> </div> </td> <td>  <div style="width:200px; height:20px;background:#dab872"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#c08775"> </div> </td> <td>  <div style="width:200px; height:20px;background:#df678a"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#e3dde1"> </div> </td> <td>  <div style="width:200px; height:20px;background:#fffffe"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#e7f074"> </div> </td> <td>  <div style="width:200px; height:20px;background:#fefe44"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#e4de34"> </div> </td> <td>  <div style="width:200px; height:20px;background:#aae252"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#60cbb2"> </div> </td> <td>  <div style="width:200px; height:20px;background:#46968b"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#612a7d"> </div> </td> <td>  <div style="width:200px; height:20px;background:#4a3d3b"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#9d19c4"> </div> </td> <td>  <div style="width:200px; height:20px;background:#8e4187"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#861a36"> </div> </td> <td>  <div style="width:200px; height:20px;background:#b94859"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#73e171"> </div> </td> <td>  <div style="width:200px; height:20px;background:#5f9177"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#3eb87f"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#76932e"> </div> </td> <td>  <div style="width:200px; height:20px;background:#58e730"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#a4d66b"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#aa9522"> </div> </td> <td>  <div style="width:200px; height:20px;background:#fe7b4f"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#56661d"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#59621c"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#206125"> </div> </td> <td>  <div style="width:200px; height:20px;background:#5d9670"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#e6dfcd"> </div> </td> <td>  <div style="width:200px; height:20px;background:#a6948f"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#9feaa2"> </div> </td> <td>  <div style="width:200px; height:20px;background:#808178"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#501d41"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#eae9cd"> </div> </td> <td>  <div style="width:200px; height:20px;background:#a6948f"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#1a1f3e"> </div> </td> <td>  <div style="width:200px; height:20px;background:#7e7d7e"> </div> </td> </tr>
<tr><td> <div style="width:200px; height:20px;background:#551b28"> </div> </td> <td>  <div style="width:200px; height:20px;background:#fe0a01"> </div> </td> </tr>
</table>

<p>While there are clear mistakes by the model, specially at the end of the list, I think in general the these is a feasible correspondence. This means,
the node representation we are learning via the graph neural net is able to encode in a good way the the information propagated by the neighboring nodes, 
in this case, the color. There are several things we can discuss from this small experiment and  its limitations.</p>

<p>For example, when we generate the graphs,  we first discretize the image into color zones, given a specified threshold. This resulting set of colors will be our nodes.
Then, as mentioned above, the edges are generated in terms of if two color zones  are contiguous. In that sense, for most parts , edges associate two colors
that are similar, or belong to the same portion of the color spectrum. But naturally there are cases were two color zones are contiguous, but their colors are totally
different. Think , for example, in the case of an image that has grass next to a road. In this case, the is a big chance that a green gets linked a grey. Having
this type of edges is part of the nature of the problem, but certainly  could be confusing the model, as one unlabeled node could be receiving  information 
that is two heterogeneous, that ultimately its generated color will be kind of random. An alternative could to just ignore edges between nodes that are too dissimilar,
but in turn that could generate a very disjoint graph, just small connected components without any inter-color transference. That is not so interesting.</p>

<p>The code to reproduce the experiments on this article will be available here:</p>

<p><a href="https://github.com/solargrammars/colors-in-context">https://github.com/solargrammars/colors-in-context</a></p>

  </article>

  <div class="post-share">
    <div class="post-date">Feel free to share!</div>
    <div class="sharing-icons">
      <a href="https://twitter.com/intent/tweet?text=Palettes and Graphs as Color Context&amp;url=/blog/colors-in-context.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      <!--<a href="https://www.facebook.com/sharer/sharer.php?u=/blog/colors-in-context.html&amp;title=Palettes and Graphs as Color Context" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
      <a href="https://plus.google.com/share?url=/blog/colors-in-context.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>-->
    </div>
  </div>
  <!--
  <div class="related">
    <h2>You may also enjoy...</h2>
    
    <ul class="related-posts">
      
        
          
            <li>
              <h3>
                <a href="/blog/aecolors.html">
                  <div class="related-thumbnail">
                    
                      <img src="http://pablomentat.duckdns.org:4000/assets/img/blog/learn-color-reps/gre_set10.png">
                    
                  </div>
                  <div class="related-title">
                    Autoencoding Color-Language Representations
                  </div>
                  
                </a>
              </h3>
            </li>
            
          
        
          
            <li>
              <h3>
                <a href="/blog/hoc-program-repair.html">
                  <div class="related-thumbnail">
                    
                      <img src="http://pablomentat.duckdns.org:4000/assets/img/blog/learn-color-reps/gre_set10.png">
                    
                  </div>
                  <div class="related-title">
                    Program Repair by Policy Learning (in progress)
                  </div>
                  
                </a>
              </h3>
            </li>
            
          
        
      
    </ul>
  </div>
  -->
  

</div>

  </div>
  <footer class="footer">
  <!--
  
    <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  
    <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  
    <a href="rss-feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  
  -->
<!--
  <div class="post-date"><a href="/">Solar Grammars</a></div>
-->
</footer>

</div>

</body>
</html>
