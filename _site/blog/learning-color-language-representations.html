<!doctype html>
<html>

<head>

  <title>
    
      Learning Color-Language Representations | Solar Grammars
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Solar Grammars" />
  <!-- RSS-v2.0
  <link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Solar Grammars | "/>
  //-->


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning Color-Language Representations | Solar Grammars</title>
<meta name="generator" content="Jekyll v3.6.3" />
<meta property="og:title" content="Learning Color-Language Representations" />
<meta name="author" content="Solar Grammars" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based on that, contribute to understanding the principles of language acquisition. In that sense, color description, or more generally, the generation of color language, is a relevant aspect for understanding the emergence of human expression." />
<meta property="og:description" content="Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based on that, contribute to understanding the principles of language acquisition. In that sense, color description, or more generally, the generation of color language, is a relevant aspect for understanding the emergence of human expression." />
<link rel="canonical" href="http://localhost:4000/blog/learning-color-language-representations.html" />
<meta property="og:url" content="http://localhost:4000/blog/learning-color-language-representations.html" />
<meta property="og:site_name" content="Solar Grammars" />
<meta property="og:image" content="http://localhost:4000/blog/learn-color-reps/gre_set10.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-22T00:00:00+09:00" />
<script type="application/ld+json">
{"description":"Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based on that, contribute to understanding the principles of language acquisition. In that sense, color description, or more generally, the generation of color language, is a relevant aspect for understanding the emergence of human expression.","@type":"BlogPosting","image":"http://localhost:4000/blog/learn-color-reps/gre_set10.png","url":"http://localhost:4000/blog/learning-color-language-representations.html","headline":"Learning Color-Language Representations","dateModified":"2019-06-22T00:00:00+09:00","datePublished":"2019-06-22T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/learning-color-language-representations.html"},"author":{"@type":"Person","name":"Solar Grammars"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

<div class="container">
  <header class="site-header">
  <h3 class="site-title">
    <!--
    <img  src="../assets/img/SG.jpg" style="width:50px;max-width:50px;float:left;margin:5px 10px 0 0;">
    -->
    <a href="/">Solar Grammars</a>
  </h3>
  <nav class="menu-list">
    
      <a href="/pages/contact.html" class="menu-link">Contact</a>
    

    
      <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    
      <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    
      <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
    
  </nav>
  <div class="dropdown">
    <button class="dropbtn"><i class="fa fa-bars" aria-hidden="true"></i></button>
    <div class="dropdown-content">
      
        <a href="/pages/contact.html" class="menu-link">Contact</a>
      

      
        <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      
        <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
      
        <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
      
    </div>
  </div>
</header>

  <div class="posts-wrapper">
    <div class="page-content">
  <h1>
    Learning Color-Language Representations
  </h1>

  <span class="post-date">
    Written on
    
      Solar Grammars
    
  </span>

  <!--
  
    <div class="featured-image">
      <img src="/assets/img/blog/learn-color-reps/gre_set10.png">
    </div>
  
  -->
  <article>
    <p>Grounded language learning tasks allow to characterize the relationships between words semantically (abstracting from their symbolic nature), and based 
on that, contribute to understanding the principles of language 
acquisition. In that sense, color description, or more generally,  the generation of color language, is a relevant aspect for  understanding the emergence of human expression.</p>

<p>The use of colors as constructs to describe situations can be seen 
in the literature. Some uses are intriguing, for example<br />
William Gibson’s Neuromancer begins with “<em>The sky above the 
port was the color of television, turned to a dead channel</em>”. What 
exactly did the author want to express here? Of course, there is a 
common agreement on the  purpose of such a sentence as a way of 
setting the mood of a cyberpunk novel, but the perception that each 
reader has certainly varies given the situatedness of language.</p>

<p>Another example is H.P. Lovecraft, who constantly uses the concept of color (or the lack of it) to describe the universes where his solitary characters are transported to:</p>

<blockquote>
  <p>A rather large congeries of iridescent, prolately spheroidal bubbles and a very much smaller polyhedron of <strong>unknown colours</strong> and rapidly shifting surface angles (The Dreams of the House of the Witch)</p>
</blockquote>

<blockquote>
  <p>The vast tomb, or temple, was an <strong>anomalous color</strong> — a nameless blue-violet shade (The Tree on the Hill)</p>
</blockquote>

<blockquote>
  <p>Light filtered down from asky of <strong>no assignable colour</strong> in baffling, contradictory directions, and played almost sentiently over what seemed to be a curved line of gigantic hieroglyphed pedestals more hexagonal than otherwise and surmounted by cloaked, ill-defined Shapes (Through the Gates of the Silver Key).</p>
</blockquote>

<p>For Lovecraft,  the inability to identify or generate a meaningful description of color seems to be the unequivocal sign that something very bad, as usual, is going to happen. Like if being able to define a color meant for an individual to have control over the understanding of the reality.</p>

<p>Besides the literary connotations, from a machine learning point of view, there is a whole line of research dedicated to this problem, which combines natural language processing, psychology and, even art theory.</p>

<p>In general, the main idea is to use a set of (description, color) pairs to train a model able to map between these modalities. The used models differ in their nature but the task remains almost the same.  For example, we can mention the work of <a href="https://aclweb.org/anthology/D16-1202">Kawakami et al.</a> which uses a character-level recurrent architecture to map a color description to an associated color. This tackles the particular problem that word-level models usually face, which is that color descriptions tend to be short, limiting the expressiveness of the models. <a href="https://web.stanford.edu/class/cs224n/reports/2762012.pdf">Bhargava et al.</a> propose an autoencoder where the latent feature representation is aimed to align with a three-dimensional color vector. <a href="https://aclweb.org/anthology/D16-1243">Moore et al.</a> focuses on the compositionality of the color descriptions by combining a neural encoder with a Fourier-based color transformer. The <a href="https://transacl.org/ojs/index.php/tacl/article/view/1142">same author</a> incorporates a context perspective by modeling the description generation in a speaker-listener setting and also has recently <a href="https://arxiv.org/pdf/1803.03917.pdf">published</a> an approach that incorporates bilingual data.</p>

<h3 id="from-language-to-colors">From language to colors</h3>

<p>Let’s start analyzing  Kawakami et al. paper, replicating the results and discussing possible directions. 
The first element to consider is the data. In this case, the authors propose to extract color-descriptions pairs from Colourlovers. Fortunately, there is an <a href="http://www.colourlovers.com/api">API</a> and even a <a href="https://github.com/elbaschid/python-colourlovers">Python client</a> and the extraction can be done with something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">colourlovers</span> <span class="kn">import</span> <span class="n">ColourLovers</span>
<span class="n">cl</span> <span class="o">=</span> <span class="n">ColourLovers</span><span class="p">()</span>
<span class="n">cont</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span><span class="mi">1000000</span><span class="p">,</span>  <span class="mi">100</span><span class="p">):</span>
  <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="c"># take it easy</span>
  <span class="n">col</span> <span class="o">=</span> <span class="n">cl</span><span class="o">.</span><span class="n">colors</span><span class="p">(</span><span class="n">num_results</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">result_offset</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
  <span class="n">cont</span><span class="o">+=</span><span class="p">[(</span><span class="n">c</span><span class="o">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">title</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">rgb</span><span class="o">.</span><span class="n">red</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">rgb</span><span class="o">.</span><span class="n">green</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">rgb</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">col</span><span class="p">]</span>
</code></pre></div></div>

<p>After some hours, I was able to get around 800.000 pairs. I applied certain filters to remove duplicates and  characters that were very infrequent, leading to a set with the following characteristics:</p>

<p><img src="/assets/img/blog/learn-color-reps/1.png" alt="char" /></p>

<p><img src="/assets/img/blog/learn-color-reps/2.png" alt="len" /></p>

<p>The associated colors are transformed from RGB to Lab format, which allows us to arrange  in a 3-dimensional space. The spatial disposition of a sample is presented in the following animated graph:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">rgb</span><span class="p">,</span> <span class="n">lab</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">colors</span><span class="p">[:</span><span class="mi">5000</span><span class="p">])</span> <span class="c"># for a sample of 5000 colors</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">lab</span><span class="p">)</span>
<span class="n">rgb</span> <span class="o">=</span> <span class="p">[</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">i</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">rgb</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="n">a</span><span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">)</span> <span class="c"># initial angle</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">rgb</span><span class="p">))</span>
<span class="n">ani</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">ani</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'lab_space_90_30fps.gif'</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s">'imagemagick'</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/blog/learn-color-reps/lab_space_90_30fps.gif" alt="3dlab" /></p>

<p>Regarding the model, it consists of two main blocks. The first one, a color description encoder, takes a description of a color in natural language and pass it through a character-level LSTM, from which the last hidden state, <script type="math/tex">h \in \mathbb{R}^{300}</script>  is used as a description learned feature vector. The second one, a feed-forward layer takes <script type="math/tex">h</script> and outputs the associated color in Lab format as <script type="math/tex">\hat{y} = \sigma(Wh+ b)</script>, with <script type="math/tex">W \in \mathbb{R}^{3 \times 300}</script> and  <script type="math/tex">b \in \mathbb{R}^3</script>.
The model tries to minimize the mean squared error between the output color and the reference using backpropagation. While I tried several optimizers, Adam was the one with a more consistent performance on  the validation set, as seen in the following comparison:</p>

<p><img src="/assets/img/blog/learn-color-reps/curves.png" alt="curves" /></p>

<p>Initial results show coherence between the generated colors and their references. If we analyze some of the color transitions as the network passes through the descriptions, we can see how intermediate colors are also aligned with the incomplete descriptions. This shows the expressive power of the character level approach, as the generation is a sequential process at a more fine grained granularity.</p>

<p>Now, let’s take a look at  how  the hidden vectors  <script type="math/tex">h</script> produced  by the recurrent encoder change as we incrementally add more characters. For example,</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'c',
'co',
'com',
'comu',
'comua',
'comuan',
'comuanz',
'comuanza',
'comuanza ',
'comuanza g',
'comuanza gr',
'comuanza gre',
'comuanza gree',
'comuanza green'
</code></pre></div></div>
<p>or</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'K',
'Ki',
'Kit',
'Kitc',
'Kitch',
'Kitche',
'Kitchen'
</code></pre></div></div>

<p>Let’s start with a more qualitative approach, by obtaining the  intermediate feature vectors for each description and visualizing them in a two-dimensional space. Hopefully, we will be able to identify certain trajectories in vector space and their relationship to the associated colors.</p>

<p>For example, for the color with the description <strong><em>strawberry kiss</em></strong>, we can obtain the vector from all its subsequences and reduce them altogether via PCA to generate the following figure:</p>

<p><img src="/assets/img/blog/learn-color-reps/strawberry_kiss.png" alt="sk" /></p>

<p>The associated transitions are presented in the following animation, where we can see that as we progress on the addition of characters, the distance between the vectors begin to decrease. This makes sense as at the beginning there is not restriction imposed by the conditional probabilities, but as we progress, the context associated to the previously seen characters narrows
the space of possible next characters.</p>

<p><img src="/assets/img/blog/learn-color-reps/sk5.gif" alt="ska" /></p>

<p>Interestingly, the colors associated to each sub-sequence also converges. One thing to notice is, for example, in the word <strong><em>strawberry</em></strong>, when we get to the subsequence <strong><em>straw</em></strong>,  the color produced is between <em>light brown</em>  and <em>pale yellow</em>, which is characteristic to a <em>straw</em> from  an agricultural perspective. As we continue adding characters, we can see how the color moves to a more red-ish spectrum, as <strong><em>strawberry</em></strong> is formed.  The same type of behavior can be seen in most of the descriptions.</p>

<p>For <strong><em>Dijon mustard</em></strong> :</p>

<p><img src="/assets/img/blog/learn-color-reps/dijon_mustard_ii.png" alt="mustard" /></p>

<p>For <strong><em>burnt blueberry</em></strong> :</p>

<p><img src="/assets/img/blog/learn-color-reps/burnt_blueberry.png" alt="burnt" /></p>

<p>A more interesting experiment is to visualize how two or more descriptions interact as their respective characters are appended. In that sense, let us assume a subset of descriptions that share a certain substring. As the generation is conditioned by the relative position of the characters, we can study the cases where the substring appears i) at the beginning of the descriptions (e.g. as <strong><em>blue</em></strong> in <strong><em>blueberry</em></strong> and <strong><em>blue ocean</em></strong>),  ii) in the middle or iii) at the end.</p>

<p>For the  fist case, let us select the substring <strong><em>gre</em></strong> and sample ten descriptions from our test set the begin with such pattern:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'gre',
'greased lips',
'greasy spoon',
'great',
'great dark spot',
'great fright',
'great green',
'great lover',
'great party favors',
'great wide tea'
</code></pre></div></div>

<p>In this case, the resulting visualization is the following. 
(It must be noted that for the case of  two or more descriptions, the images generated by PCA are not expressive enough,
as most of the points are overlapped. For these cases, t-SNE performs much better.)</p>

<p><img src="/assets/img/blog/learn-color-reps/gre_set10.png" alt="gre10" /></p>

<p>And the corresponding animation:</p>

<p><img src="/assets/img/blog/learn-color-reps/gre5.gif" alt="gre10ani" /></p>

<p>For the second case, if we select the substring <strong><em>ran</em></strong>, we obtain following sample set:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Orange Creamsicle',
'arugula granita',
'orange scribbles',
'Light Amaranth',
'goranluppo',
'my random',
'orange celosia',
'Violet Fragrance',
'Pomegranate',
'red orange'
</code></pre></div></div>

<p><img src="/assets/img/blog/learn-color-reps/ran.png" alt="gre10" /></p>

<p>For the last case, if we sample descriptions that end in <strong><em>nge</em></strong>, we obtain:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Fringe',
'Lounge',
'Orange',
'avenge',
'change',
'fringe',
'lounge',
'orange',
'sponge',
'tounge'
</code></pre></div></div>

<p><img src="/assets/img/blog/learn-color-reps/end_nge.png" alt="end_nge" /></p>

<p><img src="/assets/img/blog/learn-color-reps/end_nge_ani2.gif" alt="end_nge_ani" /></p>

<p>Visualizing the results, we can see the most interesting trajectories are generated when we define a substring  at the end, in the sense of obtaining trajectories that are more linear and encapsulated in certain zones of the vector space than in other cases.</p>

<p>If you want to train your own model, the source for transferring from color names to LAB format can be found <a href="https://github.com/pabloloyola/name-color">here</a>.</p>

<p>Anyway, in this post we analyzed  the problem of producing a color given  a specific description of it. There are several extensions we can think of, but that is left to the reader’s imagination.</p>

<h3 id="from-colors-to-language">From colors to language</h3>

<p>Now we will try to do the inverse task, which means, give a color, expressed as a three-dimensional vector, obtain its description in natural language. To do that, we will take as inspiration  the approach described in <a href="https://arxiv.org/abs/1606.03821">Monroe et al.</a> and try to obtain a working  version in Pytorch. The current implementation is available here: https://github.com/pabloloyola/color-description</p>

<p>The data we will use in the first place is the dataset from <a href="https://blog.xkcd.com/2010/05/03/color-survey-results/">Munroe</a>, which are the results of a survey where participants were asked to provide a short description of a given color. 
The actual version of the dataset is the one processed by <a href="https://aclweb.org/anthology/Q15-1008">McMahan et al.</a> and that is available for download <a href="http://paul.rutgers.edu/~bcm84/rugstk_v1.0.tar.gz">here</a></p>

<p>The original representation of the colors is the HSV format, which stands for Hue, Saturation and Value. This format is intended to more closely resemble how humans perceive color attributes. Here is a small sample from the training set:</p>

<p><img src="/assets/img/blog/learn-color-reps/munroe-data.png" alt="munroe-data" /></p>

<p>Given the nature of the data collection, there are cases where one color description is associated to several –very similar– color vectors, and also cases  where one color vector is related to more than one color description.</p>

<p>As described in the original paper, the model consists of an LSTM decoder that receives as input a color in the HSV format and the associated description, as a sequence of words.</p>

<p>At each time step the LSTM is fed with a vector resulting from concatenating the vector representation of the color  we want to model, and the embedding vector associated to the word predicted by the model in the previous step. The output state from the LSTM is fed into a softmax layer from which the next word is selected.</p>

<p>One interesting element to consider is  how to treat the color representation. We can just pass the raw three-dimensional vector as it is, assuming the the HSV space is sufficient to provide expressive representations. Another alternative is presented in the original paper, where it is proposed to transform the HSV vectors into a Fourier basis representation, resulting in  a 54-dimensional vector.</p>

<p>An initial parameter search led to feasible results. Probably because the vocabulary size is not so big (as well as the sequences not so long). This is a random list showing a subset of the test set. The first column is the actual generated color description while the second and  third column represents that reference, in terms of the description and the color in RGB format.</p>

<p><img src="/assets/img/blog/learn-color-reps/atomic.png" alt="atomic" /></p>

<p>As we can see, the model tends to output descriptions that while do not match exactly with the reference, they clearly resemble the target color. It is interesting to see the small perceptual differences between <code class="highlighter-rouge">orange</code> and <code class="highlighter-rouge">salmon</code> and  <code class="highlighter-rouge">pink</code> and <code class="highlighter-rouge">magenta</code>. 
One thing to notice in this case, is that the color descriptions are mostly formed by one word (maybe we should just call them color names in that case). This is a natural consequence of the nature of the dataset, where people were asked to be specific in their descriptions.</p>

<p>If we filter a bit and collect results  where the generated sequences have length more than one (ignoring  <code class="highlighter-rouge">pad</code>, <code class="highlighter-rouge">bos</code> and <code class="highlighter-rouge">eos</code> special tokens), we can see something like this:</p>

<p><img src="/assets/img/blog/learn-color-reps/not-atomic-blue.png" alt="not-atomic" /></p>

<p>This a sample mostly from the <code class="highlighter-rouge">blue</code> spectrum, where can see two interesing things. Firstly, the accompanying words, usually adjectives, such as <code class="highlighter-rouge">dark</code> or <code class="highlighter-rouge">light</code>, are a bit repetitive, but effective. For example, in the third instance, we can see that the reference is just  <code class="highlighter-rouge">blue</code>, but if we look at the actual color, the generated description, <code class="highlighter-rouge">light blue</code> seems to fit better. Of course,  we do not have a quantitative way to measure this as we do not have a point of reference of what is actually <code class="highlighter-rouge">blue</code>, but the model seems to learn globally such characteristics.</p>

<p>If we filter a  bit more and consider only the generated descriptions and the references have more than one token, we obtain the following list.</p>

<p><img src="/assets/img/blog/learn-color-reps/2-2-green-blue.png" alt="2-2" /></p>

<p>In this case, we see a more uniform matching between generated and reference descriptions.</p>

<p>One interesting aspect to study is the impact of the Fourier transformation of the color vectors. The following graph compares the training and testing losses, showing that in fact expanding to a 54 dimensional representation improves model performance.</p>

<p><img src="/assets/img/blog/learn-color-reps/losses_color_desc.png" alt="losses_color_desc" /></p>

<p>Having explored the word-level approach, the natural next step is to
study if we can go a more fine grained level, for example, study if we can generate description character by character.
While this could represent an extra challenge for the model, I think it could also open the door to obtain more 
diverse descriptions. Let’s see.</p>

<p>The most straightforward way to obtain a character level approach is to reuse the exsiting code and just change the tokenizer function to split the description into a sequence of characters.</p>

<p>When I  was playing with the hyperparameters, something that caught my attention was how to structure the input vector for the decoder. As we saw above, this vector is the concatenation of the color representation (raw three-dimensional vector or an expansion based on the Fourier mapping) and the embbeded vector of the word predicted in the previous step.  For the word-based level, changing the proportion of the two components did not impact on the results. Something different I experienced in the character-level method. For example, the following figure compares the training and testing losses between three runs, using vectors of 50, 100 and 200 respectively. Again, the first column represents the generated description, and the second and third , the references.</p>

<p><img src="/assets/img/blog/learn-color-reps/losses_embedding_size_character_level.png" alt="losses_embedding_size_character_level" /></p>

<p>Such difference also impact on the generated descriptions, for example:</p>

<p>Sample of generated descriptions with embedded vector of size 50:</p>

<p><img src="/assets/img/blog/learn-color-reps/size50.png" alt="size50" /></p>

<p>Sample of generated descriptions with embedded vector of size 100:</p>

<p><img src="/assets/img/blog/learn-color-reps/size100.png" alt="size100" /></p>

<p>Sample of generated descriptions with embedded vector of size 200:</p>

<p><img src="/assets/img/blog/learn-color-reps/size200.png" alt="size200" /></p>

<p>As we can see, the size of the character embedding indeed impacts on the generation results. A vector of size 50 seems to generate incomplete descriptions, meaning the it is not expressive enough to guide the generation to known descriptions.  A size of 200 totally makes the model collapse, generating the same pattern for the majority of the instances on the test set. Finally, a size of 100 seems to  provide much reliable results, in this case we can see the both the signal from the color and the embedded vector play nicely together, such as in the cases of <code class="highlighter-rouge">red</code> for <code class="highlighter-rouge">orange-brown</code> and <code class="highlighter-rouge">brown</code>  for  <code class="highlighter-rouge">khaki</code>, which means, while the model cannot always produce  the exact description, at least it is able to use in a effective way the color representation to find a feasible alternative.</p>

<p>All the code and data generation scripts can be found here:</p>

<blockquote>
  <p><a href="https://github.com/solargrammars/name_color">https://github.com/solargrammars/name_color</a></p>
</blockquote>

<blockquote>
  <p><a href="https://github.com/solargrammars/color_descriptions">https://github.com/solargrammars/color_descriptions</a></p>
</blockquote>


  </article>

  <div class="post-share">
    <div class="post-date">Feel free to share!</div>
    <div class="sharing-icons">
      <a href="https://twitter.com/intent/tweet?text=Learning Color-Language Representations&amp;url=/blog/learning-color-language-representations.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/blog/learning-color-language-representations.html&amp;title=Learning Color-Language Representations" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
      <a href="https://plus.google.com/share?url=/blog/learning-color-language-representations.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>
    </div>
  </div>
  <!--
  <div class="related">
    <h2>You may also enjoy...</h2>
    
    <ul class="related-posts">
      
        
          
            <li>
              <h3>
                <a href="/blog/color-gates.html">
                  <div class="related-thumbnail">
                    
                      <img src="http://localhost:4000/assets/img/arctic-1.jpg">
                    
                  </div>
                  <div class="related-title">
                    Combining word and character level representations from color descriptions
                  </div>
                  
                </a>
              </h3>
            </li>
            
          
        
          
            <li>
              <h3>
                <a href="/blog/grounding-color-comparatives.html">
                  <div class="related-thumbnail">
                    
                      <img src="http://localhost:4000/assets/img/arctic-1.jpg">
                    
                  </div>
                  <div class="related-title">
                    Grounding Color Comparatives
                  </div>
                  
                </a>
              </h3>
            </li>
            
          
        
      
    </ul>
  </div>
  -->
  

</div>

  </div>
  <footer class="footer">
  <!--
  
    <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  
    <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  
    <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  
  -->
<!--
  <div class="post-date"><a href="/">Solar Grammars</a></div>
-->
</footer>

</div>

</body>
</html>
