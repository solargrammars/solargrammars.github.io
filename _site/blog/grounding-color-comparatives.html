<!doctype html>
<html>

<head>

  <title>
    
      Grounding Color Comparatives | Solar Grammars
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Solar Grammars" />
  <!-- RSS-v2.0
  <link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Solar Grammars | "/>
  //-->


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Grounding Color Comparatives | Solar Grammars</title>
<meta name="generator" content="Jekyll v3.6.3" />
<meta property="og:title" content="Grounding Color Comparatives" />
<meta name="author" content="Solar Grammars" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The methods we have covered so far allowed us to learn a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when we say a given color is mustard yellow, we are basically relating our experienced notion of mustard and a reference yellow and associating them to express our perception through language. Another example can be seen when we say dark green or deep blue. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of dark green, we use our preconceived reference of what we understand as green and its dark variant. In this process, we are implicitly internalizing that the color we are perceiving is darker than the reference we have." />
<meta property="og:description" content="The methods we have covered so far allowed us to learn a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when we say a given color is mustard yellow, we are basically relating our experienced notion of mustard and a reference yellow and associating them to express our perception through language. Another example can be seen when we say dark green or deep blue. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of dark green, we use our preconceived reference of what we understand as green and its dark variant. In this process, we are implicitly internalizing that the color we are perceiving is darker than the reference we have." />
<link rel="canonical" href="http://localhost:4000/blog/grounding-color-comparatives.html" />
<meta property="og:url" content="http://localhost:4000/blog/grounding-color-comparatives.html" />
<meta property="og:site_name" content="Solar Grammars" />
<meta property="og:image" content="http://localhost:4000/arctic-1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-01-02T00:00:00+09:00" />
<script type="application/ld+json">
{"description":"The methods we have covered so far allowed us to learn a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when we say a given color is mustard yellow, we are basically relating our experienced notion of mustard and a reference yellow and associating them to express our perception through language. Another example can be seen when we say dark green or deep blue. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of dark green, we use our preconceived reference of what we understand as green and its dark variant. In this process, we are implicitly internalizing that the color we are perceiving is darker than the reference we have.","@type":"BlogPosting","image":"http://localhost:4000/arctic-1.jpg","url":"http://localhost:4000/blog/grounding-color-comparatives.html","headline":"Grounding Color Comparatives","dateModified":"2017-01-02T00:00:00+09:00","datePublished":"2017-01-02T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/grounding-color-comparatives.html"},"author":{"@type":"Person","name":"Solar Grammars"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

<div class="container">
  <header class="site-header">
  <h3 class="site-title">
    <!--
    <img  src="../assets/img/SG.jpg" style="width:50px;max-width:50px;float:left;margin:5px 10px 0 0;">
    -->
    <a href="/">Solar Grammars</a>
  </h3>
  <nav class="menu-list">
    
      <a href="/pages/contact.html" class="menu-link">Contact</a>
    

    
      <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    
      <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    
      <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
    
  </nav>
  <div class="dropdown">
    <button class="dropbtn"><i class="fa fa-bars" aria-hidden="true"></i></button>
    <div class="dropdown-content">
      
        <a href="/pages/contact.html" class="menu-link">Contact</a>
      

      
        <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      
        <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
      
        <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
      
    </div>
  </div>
</header>

  <div class="posts-wrapper">
    <div class="page-content">
  <h1>
    Grounding Color Comparatives
  </h1>

  <span class="post-date">
    Written on
    
      Solar Grammars
    
  </span>

  <!--
  
    <div class="featured-image">
      <img src="/assets/img/arctic-1.jpg">
    </div>
  
  -->
  <article>
    <p>The methods  we have covered so far allowed  us to learn  a direct mapping between colors and language. One possible limitation of those approaches is that in reality color perception is intrinsically contextual, in the sense that usually we come up with a description for a newly seen color based on a predefined reference. For example, when
we say a given color is <code class="highlighter-rouge">mustard yellow</code>, we are basically relating our experienced notion of <code class="highlighter-rouge">mustard</code>  and a reference <code class="highlighter-rouge">yellow</code> and associating them
to express our perception through language. Another example can be seen when we say <code class="highlighter-rouge">dark green</code> or <code class="highlighter-rouge">deep blue</code>. In these cases we are adding an adjective to a color based or our predefined notion of it. In that sense, in order to communicate our perception of  <code class="highlighter-rouge">dark green</code>, we use our preconceived reference of what we understand as <code class="highlighter-rouge">green</code> and its <code class="highlighter-rouge">dark</code> variant. In this process, we are implicitly internalizing that the color we are perceiving is <code class="highlighter-rouge">darker</code> than 
the reference we have.</p>

<p>It could be interesting to model or quantify the distance between a predefined reference and a newly perceived color, and how such distance condition the language we use to  describe it.</p>

<p>In an <a href="https://www.aclweb.org/anthology/P18-2125">ACL’18 paper</a>, Winn et al. proposed a way to ground color comparatives as vectors in a color space. Let’s assume we have a reference color <code class="highlighter-rouge">yellow</code> and a given target color <code class="highlighter-rouge">dark yellow</code>, both expressed as RGB points. The associated comparative in this case is <code class="highlighter-rouge">darker</code>, as naturally, <code class="highlighter-rouge">dark yellow</code>
 is <em>darker</em> than <code class="highlighter-rouge">yellow</code> :). The key idea is to represent a comparative between both colors as a vector rooted at the reference  and that points in the direction of the the target.</p>

<p>The first interesting thing about that work is the construction of an ad-hoc dataset. The authors take as starting point  the dataset from <a href="https://www.aclweb.org/anthology/Q15-1008">McMahan et al.</a> (which in turn is a processed version of the original <a href="https://blog.xkcd.com/2010/05/03/color-survey-results/">XKCD color survey</a>). This dataset associates a color description with a RGB point.</p>

<p>The authors of this paper noticed that a considerable amount of descriptions contain
adjectives, such as <code class="highlighter-rouge">light green</code> or <code class="highlighter-rouge">dark purple</code>. From those adjectives, they obtained the associated comparatives (  <code class="highlighter-rouge">light</code> -&gt; <code class="highlighter-rouge">lighter</code>, <code class="highlighter-rouge">dark</code> -&gt; <code class="highlighter-rouge">darker</code>) so they can
conform tuples like  <code class="highlighter-rouge">(green, darker, dark green)</code>, where 
the first element represent the reference color as RGB tuple, the second is the associated comparative, and third element is the target color, also expressed as a  RGB tuple.</p>

<p>Let’s take a look a some tuples and their associated points, by plotting a two dimensional reduction via T-SNE. For example, in the following figure we can see how the RGB points for <code class="highlighter-rouge">lavender</code> are positioned  in relation to its available comparatives.</p>

<p><img src="/assets/img/blog/color-comparatives/visual_analysis_lavender.png" alt="lavender" /></p>

<p>One thing we can immediately notice is that … some does not quite resemble the common notion of <code class="highlighter-rouge">lavender</code>. But that is an interesting aspect of the dataset, as such categorization comes from a real survey, i.e., it compresses the diversity of perception from a huge number of people.  I added a marker associated to the mean value of comparative (<code class="highlighter-rouge">average</code> in this case means just plain <code class="highlighter-rouge">lavender</code>) and we can easy distinguish the defined color zones. Interestingly, the <code class="highlighter-rouge">light lavender</code> seems to be quite different from the rest, but between elements in this group, we still can see distances quite considerable. If we take a look at a couple more of examples,  we see a consistent distinction:</p>

<p><code class="highlighter-rouge">rose</code>:
<img src="/assets/img/blog/color-comparatives/visual_analysis_rose.png" alt="mint" /></p>

<p><code class="highlighter-rouge">turquoise</code>:
<img src="/assets/img/blog/color-comparatives/visual_analysis_turquoise.png" alt="mint" /></p>

<p><code class="highlighter-rouge">violet</code>:
<img src="/assets/img/blog/color-comparatives/visual_analysis_violet.png" alt="peach" /></p>

<p>Something worth noticing there could be several ways to structure  a training set. In the case of the paper from Winn et al.,  instead of forming all possible reference-target pairs between RGB points, they assume that the references converge, therefore only the average values of those sets are used.</p>

<p>The model consists of a linear layer that receives the concatenation of the  reference color <script type="math/tex">r_c</script> (as RGB three-dimensional vector) and an embedding associated to the comparative <script type="math/tex">w</script> (in the paper the authors used the  Google’s 300-dimensional word2vec pretrained vectors). The resulting representation is fed to an additional layer along with the reference color and the output consists of a 3 dimensional vector <script type="math/tex">\vec{w_g}</script> that represents the direction in the color space of the vector rooted at <script type="math/tex">r_c</script> and the <em>follows</em> the direction towards the target <script type="math/tex">t_c</script>.</p>

<p>The loss function has two components, the first one computes the cosine distance between the learned vector <script type="math/tex">\vec{w_g}</script> and the vector that connects the reference and the target colors ( <script type="math/tex">\vec{tr} =  t_c - r_c</script>).
The second component compares the size of the learned vector against the size of the vector associated to the target color. With this, it is expected to modulate  the distance to the target color so that the colors in the line of the vector resemble the comparative.</p>

<p>Let’s take a look at some examples of learned comparative vectors.  In the case of the tuple <code class="highlighter-rouge">(lavender, darker, dark lavender)</code> the first figure shows the differences between the <script type="math/tex">\vec{w_g}</script> and <script type="math/tex">\vec{tr}</script> as we graph them in a 
three dimensional space. As RGB is not a well defined metric space, the colors were transformed into the LAB
space. The second figure shows the the color gradient associated to the <script type="math/tex">\vec{tr}</script> vector and the one associated to <script type="math/tex">\vec{v_g}</script>.</p>

<p><img src="/assets/img/blog/color-comparatives/vector_visualization_darklavender.png" alt="vvis" />  <img src="/assets/img/blog/color-comparatives/color_gradients_darklavender.png" alt="vvis" /></p>

<p>From the figures above, we can see that the model actually does a good job, as <script type="math/tex">\vec{w_g}</script> follows 
quite close the direction of <script type="math/tex">\vec{tr}</script>. In terms of magnitude, <script type="math/tex">\vec{wg}</script> is longer, which may say something
about the need to adjust the second term of the loss function. But in practical terms, if we look at the color
gradients, we can see that <script type="math/tex">\vec{wg}</script> also takes us to a state that we could describe as  <code class="highlighter-rouge">dark lavender</code>.</p>

<p>If we inspect other pairs, we can see the a similar behavior.</p>

<p>For <code class="highlighter-rouge">(yellow, more orange, orange yellow)</code>, we see a quite close fit in terms of the direction, while at the same time a bigger difference 
in term of the magnitudes. This disagreement seems to not make much difference regarding the colors we can obtain. 
<img src="/assets/img/blog/color-comparatives/vector_visualization_orangeyellow.png" alt="vvis" />  <img src="/assets/img/blog/color-comparatives/color_gradients_orangeyellow.png" alt="vvis" /></p>

<p>For <code class="highlighter-rouge">(mint, darker, dark mint)</code>:
<img src="/assets/img/blog/color-comparatives/vector_visualization_darkmint.png" alt="vvis" />  <img src="/assets/img/blog/color-comparatives/color_gradients_darkmint.png" alt="vvis" /></p>

<p>In general,  the method works quite well for tuples on the test set whose elements have appeared during training, which means
RGB  tuples that are in the proximity of colors already seen. Naturally, things get more interesting when we pass a tuple in which  one or more components are totally out of distribution.</p>

<p>For example, lets take a look at the tuple <code class="highlighter-rouge">(pale blue, more , very pale blue)</code>. In this case <code class="highlighter-rouge">pale blue</code> was not present 
on the training data. If we compute the color gradients using 
the grounded comparative vector, we obtain:</p>

<p><img src="/assets/img/blog/color-comparatives/unseen_ref_color_gradients_verypaleblue.png" alt="vvis" /></p>

<p>We can see a clear difference on the ending color both gradients
reach, with the one associated to <script type="math/tex">\vec{w_g}</script> getting closer to what we could consider <em>muddier</em> rather than <em>very pale</em>.</p>

<p>Another setting we can explore is when the comparative was not seen during training. For example,  for the tuple <code class="highlighter-rouge">(pink, purplish, purplish pink)</code>, we obtain:</p>

<p><img src="/assets/img/blog/color-comparatives/unseen_comp_color_gradients_purplishpink.png" alt="vvis" /></p>

<p>Here we can see how the model is having a really hard time
trying to align we the ground truth. In fact,  we can see the 
ending colors do not resemble at all, and  in the case of
the color gradient associated to <script type="math/tex">\vec{w_g}</script>, there is no sign 
of  <em>purplishness</em> :)</p>

<p>This tells us a lot about how important is for the model
to handle the comparative representations. As we reviewed above, in the original paper the vectors associated to the comparative
tokens are pretrained vectors from Google Word2vec. Of course replacing those vectors with other, more refined, pretrainied sources could allow more generalization, but I wonder if there is any way to consider out of vocabulary comparatives in a more comprehensive way.</p>

<p>One way to approach such problem could be to estimate the direction of an unseen comparative based on the semantic relationship it may  have with existing comparatives. For example, <code class="highlighter-rouge">brighter</code> and <code class="highlighter-rouge">lighter</code> , while not pointing in the exact same direction, probably are close enough. Therefore, if one of them  not appear, probably their RGB points could 
make the model be aware of their relationship, guiding it towards a feasible region in the  color space. Unfortunately, such relationship is in directly encoded in the pretrained vectors so it might be necessary to induce such bias. Besides that, I think the most interesting extension could be 
to incorporate structure  (hierarchies ? ) to the comparative relationships, maybe based on an external source. But lets keep
that as a future work for the reader.</p>

<p>All the code, data generation and  figures can be found here:</p>

<blockquote>
  <p><a href="http://github.com/solargrammars/comparative_color_descriptions">http://github.com/solargrammars/comparative_color_descriptions</a></p>
</blockquote>

  </article>

  <div class="post-share">
    <div class="post-date">Feel free to share!</div>
    <div class="sharing-icons">
      <a href="https://twitter.com/intent/tweet?text=Grounding Color Comparatives&amp;url=/blog/grounding-color-comparatives.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/blog/grounding-color-comparatives.html&amp;title=Grounding Color Comparatives" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
      <a href="https://plus.google.com/share?url=/blog/grounding-color-comparatives.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>
    </div>
  </div>
  <!--
  <div class="related">
    <h2>You may also enjoy...</h2>
    
    <ul class="related-posts">
      
        
          
            <li>
              <h3>
                <a href="/blog/hoc.html">
                  <div class="related-thumbnail">
                    
                      <img src="http://localhost:4000/assets/img/blog/learn-color-reps/gre_set10.png">
                    
                  </div>
                  <div class="related-title">
                    Program Repair by Policy Learning (in progress)
                  </div>
                  
                </a>
              </h3>
            </li>
            
          
        
          
            <li>
              <h3>
                <a href="/blog/color-gates.html">
                  <div class="related-thumbnail">
                    
                      <img src="http://localhost:4000/assets/img/arctic-1.jpg">
                    
                  </div>
                  <div class="related-title">
                    Combining Word and Character level Representations from Color Descriptions
                  </div>
                  
                </a>
              </h3>
            </li>
            
          
        
      
    </ul>
  </div>
  -->
  

</div>

  </div>
  <footer class="footer">
  <!--
  
    <a href="https://twitter.com/solargrammars" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  
    <a href="mailto:solargrammars@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  
    <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  
  -->
<!--
  <div class="post-date"><a href="/">Solar Grammars</a></div>
-->
</footer>

</div>

</body>
</html>
